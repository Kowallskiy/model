{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b9zbC6ABPBS"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jnGRzmhqBWF1"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast, BertForTokenClassification, BertForSequenceClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import TextDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9ZtmdcxBYMs"
      },
      "outputs": [],
      "source": [
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vxqkLzucfiG_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# Read the JSON file\n",
        "path = \"/content/drive/MyDrive/model/B_data.json\"\n",
        "with open(path, 'r') as f:\n",
        "    dataset = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JQin9IoNfnh-"
      },
      "outputs": [],
      "source": [
        "text, intent, ner = [], [], []\n",
        "for i in dataset:\n",
        "    text.append(i['text'])\n",
        "    intent.append(i['intent'])\n",
        "    ner.append(i['entities'].split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw-u5fu62PwD",
        "outputId": "e3926084-4ff4-4970-9bba-54b088572f39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O', 'O', 'O', 'O', 'B-DUR', 'I-DUR']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "o = ner[0]\n",
        "o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLPHaXjv1-NC",
        "outputId": "52a14168-9fcc-4e4d-aa03-d5da51615673"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\"Set', 'a', 'timer', 'for', '10', 'minutes.\"']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "o = text[0].strip().split()\n",
        "o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTUJWv92f3sx",
        "outputId": "1ef7469f-8b80-4d0e-d00f-997402c1cceb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({\"'Schedule Appointment'\",\n",
              "  \"'Schedule Meeting'\",\n",
              "  \"'Set Alarm'\",\n",
              "  \"'Set Reminder'\",\n",
              "  \"'Set Timer'\"},\n",
              " 5)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "unique_intents = set(intent)\n",
        "num_intent_labels = len(unique_intents)\n",
        "\n",
        "unique_intents, num_intent_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdoGreyqgosK",
        "outputId": "0a2fedd3-80d9-402c-a1d8-ccbe5f98b68d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'B-DATE',\n",
              "  'B-DUR',\n",
              "  'B-TASK',\n",
              "  'B-TIME',\n",
              "  'I-DATE',\n",
              "  'I-DUR',\n",
              "  'I-TASK',\n",
              "  'I-TIME',\n",
              "  'O'},\n",
              " 9)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "one_dimensional_ner = [tag for subset in ner for tag in subset ]\n",
        "unique_ner = set(one_dimensional_ner)\n",
        "num_ner_labels = len(unique_ner)\n",
        "unique_ner, num_ner_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDSp3FM-gtCb"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "ner_model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_ner_labels)\n",
        "intent_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_intent_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gIzCQacBpFzx"
      },
      "outputs": [],
      "source": [
        "label_ner_map = {'O': 0, 'B-DATE': 1, 'I-DATE': 2, 'B-TIME': 3, 'I-TIME': 4, 'B-TASK': 5, 'I-TASK': 6, 'B-DUR': 7, 'I-DUR': 8}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "t3y8NTdexh0A"
      },
      "outputs": [],
      "source": [
        "label_intent_map = {\n",
        "    \"'Schedule Appointment'\": 0,\n",
        "    \"'Schedule Meeting'\": 1,\n",
        "    \"'Set Alarm'\": 2,\n",
        "    \"'Set Reminder'\": 3,\n",
        "    \"'Set Timer'\": 4\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QvkVdQRVjuLM"
      },
      "outputs": [],
      "source": [
        "class dataset(Dataset):\n",
        "    def __init__(self, text, intent, ner, tokenizer, max_len=128):\n",
        "        self.len = len(text)\n",
        "        self.text = text\n",
        "        self.intent = intent\n",
        "        self.ner = ner\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # step 1: get the sentence, ner label, and intent_label\n",
        "        sentence = self.text[index].strip()\n",
        "        intent_label = self.intent[index].strip()\n",
        "        ner_labels = self.ner[index]\n",
        "\n",
        "        # step 2: use tokenizer to encode a sentence (includes padding/truncation up to max length)\n",
        "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" which highlights where each token starts and ends\n",
        "        encoding = self.tokenizer(\n",
        "            sentence,\n",
        "            return_offsets_mapping=True,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_len\n",
        "        )\n",
        "\n",
        "        # step 3: create ner token labels only for first word pieces of each tokenized word\n",
        "        tokenized_ner_labels = [label_ner_map[label] for label in ner_labels]\n",
        "        # create an empty array of -100 of length max_length\n",
        "        encoded_ner_labels = np.ones(len(encoding['offset_mapping']), dtype=int) * -100\n",
        "\n",
        "        # set only labels whose first offset position is 0 and the second is not 0\n",
        "        i = 0\n",
        "        prev = -1\n",
        "        for idx, mapping in enumerate(encoding['offset_mapping']):\n",
        "            if mapping[0] == mapping[1] == 0:\n",
        "                continue\n",
        "            if mapping[0] != prev:\n",
        "                # overwrite label\n",
        "                encoded_ner_labels[idx] = tokenized_ner_labels[i]\n",
        "                prev = mapping[1]\n",
        "                i += 1\n",
        "            else:\n",
        "                prev = mapping[1]\n",
        "\n",
        "        # create intent token labels\n",
        "        tokenized_intent_label = label_intent_map[intent_label]\n",
        "\n",
        "        # step 4: turn everything into Pytorch tensors\n",
        "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
        "        item['ner_labels'] = torch.as_tensor(encoded_ner_labels)\n",
        "        item['intent_labels'] = torch.as_tensor(tokenized_intent_label)\n",
        "\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cEaNyuELzNWa"
      },
      "outputs": [],
      "source": [
        "training_set = dataset(text, intent, ner, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gqvwYgr0jfS",
        "outputId": "485ba00c-5d7f-4d53-9eb5-a97364561c26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([  101,  1000,  2275,  1037, 25309,  2005,  2184,  2781,  1012,  1000,\n",
              "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
              " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'offset_mapping': tensor([[ 0,  0],\n",
              "         [ 0,  1],\n",
              "         [ 1,  4],\n",
              "         [ 5,  6],\n",
              "         [ 7, 12],\n",
              "         [13, 16],\n",
              "         [17, 19],\n",
              "         [20, 27],\n",
              "         [27, 28],\n",
              "         [28, 29],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0]]),\n",
              " 'ner_labels': tensor([-100,    0, -100,    0,    0,    0,    7,    8, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100]),\n",
              " 'intent_labels': tensor(4)}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "training_set[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbBNIl17mTVH"
      },
      "source": [
        "Let us verify that the input ids and corresponding targets are correct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Tf1jeZb0k-U"
      },
      "outputs": [],
      "source": [
        "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0]['input_ids']), training_set[0]['ner_labels']):\n",
        "    print(f\"{token} -- {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd80tO8InMUM"
      },
      "source": [
        "# I HAVE TO GET RID OF \" IN MY DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "k8gK0rcxnQgi"
      },
      "outputs": [],
      "source": [
        "# The dataset is small, batch_size of 1 would not impact the training time significantly\n",
        "training_loader = DataLoader(training_set, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tjIswaA6ojMQ"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf352WfopY0y"
      },
      "source": [
        "The initial loss of the model should be close to -ln(1/num_labels)=-ln(1/9). In this case it is 2.20.\n",
        "Why? Because we are using cross entropy loss. The cross entropy loss is defined as -ln(probability score of the model for the correct class). In the beginning, the weights are random, so the probability distribution for all of the classes for a given token will be uniform, meaning that the probability for the correct class will be near 1/9. The loss for a given token will thus be -ln(1/9). As PyTorch's CrossEntropyLoss (which is used by BertForTokenClassification) uses mean reduction by default, it will compute the mean loss for each of the tokens in the sequence for which a label is provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jjk1I1coLLI",
        "outputId": "cb9c46eb-6129-4512-b221-35520a36eed0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.2348, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "inputs = training_set[2]\n",
        "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
        "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
        "labels = inputs[\"ner_labels\"].unsqueeze(0)\n",
        "\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "outputs = ner_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "initial_loss = outputs[0]\n",
        "initial_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iJ_clHk6ldm"
      },
      "source": [
        "The shape of logits must be __[batch_size, sequence_length, num_labels]__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "619BrUfmqBKS",
        "outputId": "86680a19-2e20-4d22-d308-902bf3b03c2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "tr_logits = outputs[1]\n",
        "tr_logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDZfSrT-7RdO"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam([\n",
        "    {'params': ner_model.parameters()},\n",
        "    {'params': intent_model.parameters()}\n",
        "], lr=1e-5)"
      ],
      "metadata": {
        "id": "0Jfa3o8tUfLU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lHgjJq1pqHRP"
      },
      "outputs": [],
      "source": [
        "optimizer1 = torch.optim.Adam(ner_model.parameters(), lr=1e-5)\n",
        "optimizer2 = torch.optim.Adam(intent_model.parameters(), lr=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TxB_EoOXvWm"
      },
      "source": [
        "# Just testing the models; delete it after finished"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tr_loss1 = 0\n",
        "nb_tr_steps = 0\n",
        "\n",
        "ner_model.train()\n",
        "intent_model.train()\n",
        "\n",
        "for idx, batch in enumerate(training_loader):\n",
        "    ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "    mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "    ner_labels = batch['ner_labels'].to(device, dtype=torch.long)\n",
        "    intent_labels = batch['intent_labels'].to(device, dtype=torch.long)\n",
        "\n",
        "    optimizer1.zero_grad()\n",
        "\n",
        "    ner_logits = ner_model(input_ids=ids, attention_mask=mask, labels=ner_labels)\n",
        "    ner_loss = ner_logits.loss\n",
        "\n",
        "    tr_loss1 += ner_logits.loss.item()\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "    if idx % 1 == 0:\n",
        "        loss_step = (tr_loss1) / nb_tr_steps\n",
        "        print(f\"Training loss per {idx} training steps: {loss_step}\")\n",
        "\n",
        "\n",
        "    ner_loss.backward(retain_graph=True)\n",
        "    optimizer1.step()"
      ],
      "metadata": {
        "id": "gWuNo4IbVRj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tArEsA3RnQPg"
      },
      "outputs": [],
      "source": [
        "tr_loss1, tr_loss2, tr_accuracy = 0, 0, 0\n",
        "nb_tr_steps = 0\n",
        "tr_ner_preds, tr_ner_labels = [], []\n",
        "tr_intent_labels, tr_intent_predictions = [], []\n",
        "ner_model.train()\n",
        "intent_model.train()\n",
        "\n",
        "for idx, batch in enumerate(training_loader):\n",
        "    ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "    mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "    ner_labels = batch['ner_labels'].to(device, dtype=torch.long)\n",
        "    intent_labels = batch['intent_labels'].to(device, dtype=torch.long)\n",
        "\n",
        "    ner_logits = ner_model(input_ids=ids, attention_mask=mask, labels=ner_labels)\n",
        "\n",
        "    # here we train an intent_model\n",
        "    intent_logits = intent_model(input_ids=ids, attention_mask=mask, labels=intent_labels)\n",
        "\n",
        "    # till here\n",
        "\n",
        "    tr_loss1 += ner_logits['loss']\n",
        "    tr_loss2 += intent_logits['loss']\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "    if idx % 1 == 0:\n",
        "        loss_step = (tr_loss1 + tr_loss2) / nb_tr_steps\n",
        "        print(f\"Training loss per {idx} training steps: {loss_step}\")\n",
        "\n",
        "    optimizer1.zero_grad()\n",
        "    optimizer2.zero_grad()\n",
        "    tr_loss1.backward(retain_graph=True)\n",
        "    tr_loss2.backward(retain_graph=True)\n",
        "    optimizer1.step()\n",
        "    optimizer2.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az-xZG1LX2eG",
        "outputId": "61f17d11-4d4f-44ab-b143-f06f4dad445d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss per 0 training steps: 1.9167144298553467\n",
            "Training loss per 1 training steps: 2.205728054046631\n",
            "Training loss per 2 training steps: 2.337467908859253\n",
            "Training loss per 3 training steps: 2.2807841300964355\n",
            "Training loss per 4 training steps: 2.338876247406006\n",
            "Training loss per 5 training steps: 2.3052432537078857\n",
            "Training loss per 6 training steps: 2.4021623134613037\n",
            "Training loss per 7 training steps: 2.4296467304229736\n",
            "Training loss per 8 training steps: 2.421961784362793\n",
            "Training loss per 9 training steps: 2.4600260257720947\n",
            "Training loss per 10 training steps: 2.4135680198669434\n",
            "Training loss per 11 training steps: 2.398432970046997\n",
            "Training loss per 12 training steps: 2.449944257736206\n",
            "Training loss per 13 training steps: 2.46653151512146\n",
            "Training loss epoch: 2.46653151512146\n",
            "Training accuracy epoch: 0.9254406307977737\n"
          ]
        }
      ],
      "source": [
        "tr_loss, tr_accuracy = 0, 0\n",
        "nb_tr_steps = 0\n",
        "tr_ner_preds, tr_ner_labels = [], []\n",
        "tr_intent_labels, tr_intent_predictions = [], []\n",
        "ner_model.train()\n",
        "intent_model.train()\n",
        "\n",
        "for idx, batch in enumerate(training_loader):\n",
        "    ner_model.train()\n",
        "    intent_model.train()\n",
        "    ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "    mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "    ner_labels = batch['ner_labels'].to(device, dtype=torch.long)\n",
        "    intent_labels = batch['intent_labels'].to(device, dtype=torch.long)\n",
        "\n",
        "    ner_logits = ner_model(input_ids=ids, attention_mask=mask, labels=ner_labels)\n",
        "\n",
        "    # here we train an intent_model\n",
        "    intent_logits = intent_model(input_ids=ids, attention_mask=mask, labels=intent_labels)\n",
        "\n",
        "    ner_loss = ner_logits.loss\n",
        "    intent_loss = intent_logits.loss\n",
        "\n",
        "    comb_loss = ner_loss + intent_loss\n",
        "    # till here\n",
        "\n",
        "    tr_loss += ner_logits['loss'] + intent_logits['loss']\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "    if idx % 1 == 0:\n",
        "        loss_step = tr_loss / nb_tr_steps\n",
        "        print(f\"Training loss per {idx} training steps: {loss_step}\")\n",
        "\n",
        "    # compute training accuracy (FOR NER)\n",
        "    flattened_ner_targets = ner_labels.view(-1) # shape (batch_size * seq_len)\n",
        "    active_ner_logits = ner_logits.logits.view(-1, ner_model.num_labels) # shape (batch_size*seq_len, num_labels)\n",
        "    flattened_ner_predictions = torch.argmax(active_ner_logits, axis=1) # shape (batch_size * seq_len)\n",
        "\n",
        "    # compute accuracy only at active labels\n",
        "    active_ner_accuracy = ner_labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "    ac_ner_labels = torch.masked_select(flattened_ner_targets, active_ner_accuracy)\n",
        "    ner_predictions = torch.masked_select(flattened_ner_predictions, active_ner_accuracy)\n",
        "\n",
        "    tr_ner_labels.extend(ac_ner_labels)\n",
        "    tr_ner_preds.extend(ner_predictions)\n",
        "\n",
        "    # compute accuracy for intent_model\n",
        "    # I CAN MAKE THE CALCULATION MUCH EASIER\n",
        "    # FIGURE IT OUT\n",
        "    flattened_intent_targets = intent_labels.view(-1)\n",
        "    active_intent_logits = intent_logits.logits.view(-1, intent_model.num_labels)\n",
        "    flattened_intent_predictions = torch.argmax(active_intent_logits, axis=1)\n",
        "\n",
        "    sample_intent_accuracy = intent_labels.view(-1)\n",
        "    active_intent_accuracy = torch.ones_like(sample_intent_accuracy, dtype=torch.bool)\n",
        "\n",
        "    ac_intent_labels = torch.masked_select(flattened_intent_targets, active_intent_accuracy)\n",
        "    intent_predictions = torch.masked_select(flattened_intent_predictions, active_intent_accuracy)\n",
        "\n",
        "    tr_intent_labels.extend(ac_intent_labels)\n",
        "    tr_intent_predictions.extend(intent_predictions)\n",
        "\n",
        "    tmp_tr_ner_accuracy = accuracy_score(ac_ner_labels.cpu().numpy(), ner_predictions.cpu().numpy())\n",
        "    tmp_tr_intent_accuracy = accuracy_score(ac_intent_labels.cpu().numpy(), intent_predictions.cpu().numpy())\n",
        "    tr_accuracy += tmp_tr_ner_accuracy + tmp_tr_intent_accuracy\n",
        "\n",
        "    # gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(\n",
        "        parameters=ner_model.parameters(), max_norm=10\n",
        "    )\n",
        "\n",
        "    # backward pass\n",
        "    optimizer.zero_grad()\n",
        "    comb_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "epoch_loss = tr_loss / nb_tr_steps\n",
        "tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "print(f\"Training loss epoch: {epoch_loss}\")\n",
        "print(f\"Training accuracy epoch: {tr_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kISPCk3X1VU"
      },
      "source": [
        "____"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_PxBooK8cdo"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_steps = 0\n",
        "    tr_ner_preds, tr_ner_labels = [], []\n",
        "    tr_intent_labels, tr_intent_predictions = [], []\n",
        "    ner_model.train()\n",
        "    intent_model.train()\n",
        "\n",
        "    for idx, batch in enumerate(training_loader):\n",
        "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "        ner_labels = batch['ner_labels'].to(device, dtype=torch.long)\n",
        "        intent_labels = batch['intent_labels'].to(device, dtype=torch.long)\n",
        "\n",
        "        ner_logits = ner_model(input_ids=ids, attention_mask=mask, labels=ner_labels)\n",
        "\n",
        "        # here we train an intent_model\n",
        "        intent_logits = intent_model(input_ids=ids, attention_mask=mask, labels=intent_labels)\n",
        "\n",
        "        ner_loss = ner_logits.loss\n",
        "        intent_loss = intent_logits.loss\n",
        "\n",
        "        comb_loss = ner_loss + intent_loss\n",
        "        # till here\n",
        "\n",
        "        tr_loss += ner_logits['loss'] + intent_logits['loss']\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "        if idx % 1 == 0:\n",
        "            loss_step = tr_loss / nb_tr_steps\n",
        "            print(f\"Training loss per {idx} training steps: {loss_step}\")\n",
        "\n",
        "        # compute training accuracy (FOR NER)\n",
        "        flattened_ner_targets = ner_labels.view(-1) # shape (batch_size * seq_len)\n",
        "        active_ner_logits = ner_logits.logits.view(-1, ner_model.num_labels) # shape (batch_size*seq_len, num_labels)\n",
        "        flattened_ner_predictions = torch.argmax(active_ner_logits, axis=1) # shape (batch_size * seq_len)\n",
        "\n",
        "        # compute accuracy only at active labels\n",
        "        active_ner_accuracy = ner_labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        ac_ner_labels = torch.masked_select(flattened_ner_targets, active_ner_accuracy)\n",
        "        ner_predictions = torch.masked_select(flattened_ner_predictions, active_ner_accuracy)\n",
        "\n",
        "        tr_ner_labels.extend(ac_ner_labels)\n",
        "        tr_ner_preds.extend(ner_predictions)\n",
        "\n",
        "        # compute accuracy for intent_model\n",
        "        # I CAN MAKE THE CALCULATION MUCH EASIER\n",
        "        # FIGURE IT OUT\n",
        "        flattened_intent_targets = intent_labels.view(-1)\n",
        "        active_intent_logits = intent_logits.logits.view(-1, intent_model.num_labels)\n",
        "        flattened_intent_predictions = torch.argmax(active_intent_logits, axis=1)\n",
        "\n",
        "        sample_intent_accuracy = intent_labels.view(-1)\n",
        "        active_intent_accuracy = torch.ones_like(sample_intent_accuracy, dtype=torch.bool)\n",
        "\n",
        "        ac_intent_labels = torch.masked_select(flattened_intent_targets, active_intent_accuracy)\n",
        "        intent_predictions = torch.masked_select(flattened_intent_predictions, active_intent_accuracy)\n",
        "\n",
        "        tr_intent_labels.extend(ac_intent_labels)\n",
        "        tr_intent_predictions.extend(intent_predictions)\n",
        "\n",
        "        tmp_tr_ner_accuracy = accuracy_score(ac_ner_labels.cpu().numpy(), ner_predictions.cpu().numpy())\n",
        "        tmp_tr_intent_accuracy = accuracy_score(ac_intent_labels.cpu().numpy(), intent_predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_ner_accuracy + tmp_tr_intent_accuracy\n",
        "\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=ner_model.parameters(), max_norm=10\n",
        "        )\n",
        "\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        comb_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}