{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "lJDVuWO3B1pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CptExsPx7v1M"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForTokenClassification, BertForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import TextDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "id": "0xAaLdBZE0G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# Read the JSON file\n",
        "path = \"/content/drive/MyDrive/model/B_data.json\"\n",
        "with open(path, 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Assuming the dataset is a list of dictionaries\n",
        "\n",
        "# Define a custom Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Create an instance of CustomDataset\n",
        "custom_dataset = CustomDataset(dataset)\n",
        "\n",
        "# Create a DataLoader\n",
        "batch_size = 1\n",
        "train_dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "bXc4PNVFRP4D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = []\n",
        "for sample in train_dataloader:\n",
        "    key.append(sample['entities'])"
      ],
      "metadata": {
        "id": "sMcD55taOmXE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities = []\n",
        "for substr in key:\n",
        "    tokens = substr[0].split()\n",
        "    entities.extend(tokens)"
      ],
      "metadata": {
        "id": "RuS23pIA8o0K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities"
      ],
      "metadata": {
        "id": "KRXA6PeKCy4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = set(entities)\n",
        "print(key)\n",
        "print(len(key))\n",
        "num_ner_labels = len(key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJB6otmMQSDG",
        "outputId": "6d94fe5e-fa30-4149-c5f1-7d3a9f6c3163"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'B-DATE', 'B-TIME', 'B-DUR', 'O', 'I-DUR', 'I-TASK', 'B-TASK', 'I-DATE', 'I-TIME'}\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = []\n",
        "for sample in train_dataloader:\n",
        "    l.extend(sample['intent'])"
      ],
      "metadata": {
        "id": "UY9epjj3RVri"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = set(l)\n",
        "print(l)\n",
        "print(len(l))\n",
        "num_intent_labels=len(l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAA0ljZPH0oL",
        "outputId": "e36b2467-396d-4896-def3-e2e62a7af948"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"'Schedule Meeting'\", \"'Set Alarm'\", \"'Schedule Appointment'\", \"'Set Timer'\", \"'Set Reminder'\"}\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT tokenizer and models for token classification and sequence classification\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "ner_model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_ner_labels)\n",
        "intent_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_intent_labels)"
      ],
      "metadata": {
        "id": "SZaMCTlI71p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing the dataset"
      ],
      "metadata": {
        "id": "-odMqRP2VICq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maybe I should try to set trincation and padding to False."
      ],
      "metadata": {
        "id": "n7tTgv64JFcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs = []\n",
        "\n",
        "max_sequence_length = 0  # Initialize maximum sequence length\n",
        "\n",
        "for batch in train_dataloader:\n",
        "    text = batch['text']\n",
        "    batch_tokenized = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "    input_ids = batch_tokenized['input_ids']\n",
        "    attention_mask = batch_tokenized['attention_mask']\n",
        "    tokenized_inputs.append((input_ids, attention_mask))\n",
        "\n",
        "    # Update maximum sequence length\n",
        "    max_sequence_length = max(max_sequence_length, input_ids.shape[1])\n",
        "\n",
        "# Pad sequences to the maximum sequence length\n",
        "for i in range(len(tokenized_inputs)):\n",
        "    input_ids, attention_mask = tokenized_inputs[i]\n",
        "    pad_length = max_sequence_length - input_ids.shape[1]\n",
        "    padded_input_ids = torch.nn.functional.pad(input_ids, (0, pad_length), value=tokenizer.pad_token_id)\n",
        "    padded_attention_mask = torch.nn.functional.pad(attention_mask, (0, pad_length), value=0)  # Assuming 0 for padding mask\n",
        "    tokenized_inputs[i] = (padded_input_ids, padded_attention_mask)\n",
        "\n",
        "# Concatenate input IDs and attention masks separately\n",
        "input_ids = torch.cat([tensor[0] for tensor in tokenized_inputs], dim=0)\n",
        "attention_mask = torch.cat([tensor[1] for tensor in tokenized_inputs], dim=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "DGFDqLQTVHcL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids"
      ],
      "metadata": {
        "id": "VpCPdPOoJAqG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7783edec-009c-4561-80a0-2d21842ee763"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  101,  1000,  2275,  1037, 25309,  2005,  2184,  2781,  1012,  1000,\n",
              "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  1000, 10825,  2033,  2055,  1996,  3116,  2012,  1017,  7610,\n",
              "          4826,  1012,  1000,   102,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  1000,  6134,  2019,  6098,  2005,  2279,  5958,  2012,  1023,\n",
              "          2572,  1012,  1000,   102,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  1000,  2064,  2017,  2275,  1037, 14764,  2005,  2026,  3460,\n",
              "          1005,  1055,  6098,  2006,  6928,  1029,  1000,   102,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  1000,  1045,  2215,  2000,  6134,  1037,  3116,  2005,  1996,\n",
              "          6286,  1997,  2023,  3204,  2012,  1016,  1024,  2382,  7610,  1012,\n",
              "          1000,   102],\n",
              "        [  101,  1000,  2275,  2019,  8598,  2005,  1021,  2572,  1012,  1000,\n",
              "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  1000, 10825,  2033,  2000,  2655,  2198,  1999,  2382,  2781,\n",
              "          1012,  1000,   102,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  1000,  6134,  1037,  3116,  2005,  2279,  9317,  5027,  1012,\n",
              "          1000,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  1000,  2064,  2017,  2275,  1037, 25309,  2005,  8434,  2005,\n",
              "          1015,  3178,  1029,  1000,   102,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  1000, 10825,  2033,  2055,  1996,  2622, 15117,  2012,  1019,\n",
              "          7610,  2006,  5958,  1012,  1000,   102,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  1000,  6134,  1037,  3460,  1005,  1055,  6098,  2005,  2233,\n",
              "          3983,  2012,  2184,  1024,  2382,  2572,  1012,  1000,   102,     0,\n",
              "             0,     0],\n",
              "        [  101,  1000,  2275,  1037, 25309,  2005,  1037,  2321,  1011,  3371,\n",
              "          3338,  1012,  1000,   102,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  1000, 10825,  2033,  2000,  4965, 26298,  4826,  2851,  1012,\n",
              "          1000,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  1000,  6134,  1037,  3034,  2655,  2005,  1996,  2034,  6928,\n",
              "          1997,  2279,  3204,  2012,  1017,  7610,  1012,  1000,   102,     0,\n",
              "             0,     0]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# task -> B-TASK\n",
        "# date -> B-DATE\n",
        "# duration -> B-DUR\n",
        "# days -> B_DAY\n",
        "# time -> B-TIME\n",
        "# specific_time\n",
        "encoded_ner_labels = []\n",
        "label_ner_map = {'O': 0, 'B-DATE': 1, 'I-DATE': 2, 'B-TIME': 3, 'I-TIME': 4, 'B-TASK': 5, 'I-TASK': 6, 'B-DUR': 7, 'I-DUR': 8}\n",
        "for sample in train_dataloader:\n",
        "    encoded_ner_labels.extend([label_ner_map[label] for label in sent_label.split()] for sent_label in sample['entities'])\n",
        "encoded_ner_labels"
      ],
      "metadata": {
        "id": "Y6J-hyyTpSAD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ad0956-fd0b-4304-9687-9d671f484d04"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 7, 8],\n",
              " [0, 0, 0, 0, 0, 0, 3, 4, 1],\n",
              " [0, 0, 0, 0, 1, 2, 0, 3, 4],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 3, 4],\n",
              " [0, 0, 0, 0, 3, 4],\n",
              " [0, 0, 0, 5, 6, 0, 7, 8],\n",
              " [0, 0, 0, 0, 1, 2, 3],\n",
              " [0, 0, 0, 0, 0, 0, 5, 0, 7, 8],\n",
              " [0, 0, 0, 0, 5, 6, 0, 3, 4, 0, 1],\n",
              " [0, 0, 0, 0, 0, 1, 2, 0, 3, 4],\n",
              " [0, 0, 0, 0, 0, 3, 5],\n",
              " [0, 0, 0, 5, 6, 1, 3],\n",
              " [0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0, 3, 4]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_dataloader:\n",
        "    print(i['entities'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db3lumxzUty7",
        "outputId": "e9a90ad9-e6ab-4084-b3cf-ce9310512645"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O O O O B-DUR I-DUR']\n",
            "['O O O O O O B-TIME I-TIME B-DATE']\n",
            "['O O O O B-DATE I-DATE O B-TIME I-TIME']\n",
            "['O O O O O O O O O O B-DATE']\n",
            "['O O O O O O O O B-DATE I-DATE I-DATE I-DATE O B-TIME I-TIME']\n",
            "['O O O O B-TIME I-TIME']\n",
            "['O O O B-TASK I-TASK O B-DUR I-DUR']\n",
            "['O O O O B-DATE I-DATE B-TIME']\n",
            "['O O O O O O B-TASK O B-DUR I-DUR']\n",
            "['O O O O B-TASK I-TASK O B-TIME I-TIME O B-DATE']\n",
            "['O O O O O B-DATE I-DATE O B-TIME I-TIME']\n",
            "['O O O O O B-TIME B-TASK']\n",
            "['O O O B-TASK I-TASK B-DATE B-TIME']\n",
            "['O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O B-TIME I-TIME']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "sample_entities = []\n",
        "for sample in train_dataloader:\n",
        "    sample_entities.extend(sample['entities'])\n",
        "\n",
        "# Flatten the list of strings into a single list of tokens\n",
        "all_entities = [entity for sample in sample_entities for entity in sample.split()]\n",
        "print(all_entities)\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit LabelEncoder to all unique entity labels\n",
        "label_encoder.fit(all_entities)\n",
        "\n",
        "# Encode entities for each sample in sample_entities\n",
        "encoded_entities = [\n",
        "    label_encoder.transform(entity.split()).tolist() for entity in sample_entities\n",
        "]\n",
        "\n",
        "print(encoded_entities)\n"
      ],
      "metadata": {
        "id": "TdU2s6l0pUHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a540f67-101a-4f97-d265-01c867473444"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'O', 'O', 'O', 'B-DUR', 'I-DUR', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'I-TIME', 'B-DATE', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'B-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'B-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'B-TIME', 'I-TIME', 'O', 'O', 'O', 'B-TASK', 'I-TASK', 'O', 'B-DUR', 'I-DUR', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TASK', 'O', 'B-DUR', 'I-DUR', 'O', 'O', 'O', 'O', 'B-TASK', 'I-TASK', 'O', 'B-TIME', 'I-TIME', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'B-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'B-TASK', 'O', 'O', 'O', 'B-TASK', 'I-TASK', 'B-DATE', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'B-TIME', 'I-TIME']\n",
            "[[8, 8, 8, 8, 1, 5], [8, 8, 8, 8, 8, 8, 3, 7, 0], [8, 8, 8, 8, 0, 4, 8, 3, 7], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0], [8, 8, 8, 8, 8, 8, 8, 8, 0, 4, 4, 4, 8, 3, 7], [8, 8, 8, 8, 3, 7], [8, 8, 8, 2, 6, 8, 1, 5], [8, 8, 8, 8, 0, 4, 3], [8, 8, 8, 8, 8, 8, 2, 8, 1, 5], [8, 8, 8, 8, 2, 6, 8, 3, 7, 8, 0], [8, 8, 8, 8, 8, 0, 4, 8, 3, 7], [8, 8, 8, 8, 8, 3, 2], [8, 8, 8, 2, 6, 0, 3], [8, 8, 8, 8, 8, 8, 0, 4, 4, 4, 4, 8, 3, 7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_intent_labels = []\n",
        "for sample in train_dataloader:\n",
        "    sample_intent_labels.extend(sample['intent'])\n",
        "\n",
        "label_encoder.fit(sample_intent_labels)\n",
        "\n",
        "encoded_intent_labels = label_encoder.transform(sample_intent_labels)\n",
        "\n",
        "encoded_intent_labels"
      ],
      "metadata": {
        "id": "_fSSwvVtI1MV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe5e7e66-77a7-44fb-eb85-65c154d81610"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 3, 0, 3, 1, 2, 3, 1, 4, 3, 0, 4, 3, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenizedDataset(Dataset):\n",
        "    def __init__(self, tokenized_inputs, encoded_ner_labels, encoded_intent_labels):\n",
        "        self.tokenized_inputs = tokenized_inputs\n",
        "        self.encoded_ner_labels = encoded_ner_labels\n",
        "        self.encoded_intent_labels = encoded_intent_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inputs = self.tokenized_inputs[idx]\n",
        "        ner_labels = self.encoded_ner_labels[idx]\n",
        "        intent_labels = self.encoded_intent_labels[idx]\n",
        "\n",
        "        return {\n",
        "            'inputs': inputs,\n",
        "            'ner_labels': ner_labels,\n",
        "            'intent_labels': intent_labels\n",
        "        }"
      ],
      "metadata": {
        "id": "LqdVj_pZo6P6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cust_dataset = TokenizedDataset(input_ids, encoded_ner_labels, encoded_intent_labels)\n",
        "data_loader = DataLoader(cust_dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "FSIViFHI1b48"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ner_model architecture\n",
        "ner_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TafoSlh3S8Bn",
        "outputId": "99626944-db75-4499-c23b-708e8806fbfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intent_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6niOzBtQS_Am",
        "outputId": "aace6bb3-584e-4a82-b0b1-43d9d6125bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in data_loader:\n",
        "    print(sample['ner_labels'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO7vbAMi0R7H",
        "outputId": "34541049-e532-4d9c-c469-5054fc4ed288"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([3]), tensor([4]), tensor([1])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([2]), tensor([0]), tensor([3]), tensor([4])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([7]), tensor([8])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([2]), tensor([2]), tensor([2]), tensor([0]), tensor([3]), tensor([4])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([5]), tensor([6]), tensor([0]), tensor([3]), tensor([4]), tensor([0]), tensor([1])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([5]), tensor([6]), tensor([0]), tensor([7]), tensor([8])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([2]), tensor([0]), tensor([3]), tensor([4])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([5]), tensor([0]), tensor([7]), tensor([8])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([3]), tensor([4])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([3]), tensor([5])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([0]), tensor([3]), tensor([4])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([2]), tensor([3])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([5]), tensor([6]), tensor([1]), tensor([3])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam([\n",
        "    {'params': ner_model.parameters()},\n",
        "    {'params': intent_model.parameters()}\n",
        "], lr=1e-5)  # Define optimizer for both models\n",
        "\n",
        "num_epochs = 3  # Define number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in data_loader:  # Iterate over your dataset batches\n",
        "\n",
        "        # Forward pass for intent classification\n",
        "        outputs_intent = intent_model(batch['inputs'])\n",
        "        intent_labels = batch['intent_labels']\n",
        "        intent_loss = torch.nn.CrossEntropyLoss()(outputs_intent.logits, intent_labels)\n",
        "\n",
        "        # Forward pass for NER\n",
        "        outputs_ner_list = ner_model(batch['inputs'])\n",
        "\n",
        "        # Calculate NER loss for each output tensor separately\n",
        "        ner_losses = []\n",
        "        for outputs_ner in outputs_ner_list:\n",
        "            ner_loss = torch.nn.CrossEntropyLoss()(outputs_ner.logits.view(-1, num_ner_labels), batch['ner_labels'].view(-1))\n",
        "            ner_losses.append(ner_loss)\n",
        "\n",
        "        # Sum all individual NER losses\n",
        "        ner_loss = sum(ner_losses)\n",
        "\n",
        "        # Total loss (you might adjust weights for different tasks)\n",
        "        total_loss = ner_loss + intent_loss\n",
        "\n",
        "        # Backpropagation\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "h9JGFaBu8wGT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "4d29c3f7-585e-45d6-8948-699c5ad34f58"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-77f387866141>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mner_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0moutputs_ner\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs_ner_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mner_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_ner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_ner_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ner_labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mner_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'logits'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam([\n",
        "    {'params': ner_model.parameters()},\n",
        "    {'params': intent_model.parameters()}\n",
        "], lr=1e-5)  # Define optimizer for both models\n",
        "\n",
        "num_epochs = 3  # Define number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:  # Iterate over your dataset batches\n",
        "        inputs = tokenizer(batch['text'], return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "        # Forward pass for intent classification\n",
        "        outputs_intent = intent_model(**inputs)\n",
        "        intent_labels = batch['intent']\n",
        "        intent_loss = torch.nn.CrossEntropyLoss()(outputs_intent.logits, intent_labels)\n",
        "\n",
        "        # Forward pass for NER\n",
        "        outputs_ner = ner_model(**inputs)\n",
        "        ner_labels = batch['entities']\n",
        "        ner_loss = torch.nn.CrossEntropyLoss()(outputs_ner.logits.view(-1, num_ner_labels), ner_labels.view(-1))\n",
        "\n",
        "        # Total loss (you might adjust weights for different tasks)\n",
        "        total_loss = ner_loss + intent_loss\n",
        "\n",
        "        # Backpropagation\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        # No idea yet"
      ],
      "metadata": {
        "id": "MZBdetDs9xfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the Multi-Task Architecture"
      ],
      "metadata": {
        "id": "OZ5Jw2arAaVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskBertWrapper(nn.Module):\n",
        "    def __init__(self, ner_model, intent_model):\n",
        "        super().__init__()\n",
        "        self.ner_model = ner_model\n",
        "        self.intent_model = intent_model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None, task_type=None):\n",
        "        if task_type = 'ner':\n",
        "            self.ner_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        elif task_type = 'intent':\n",
        "            self.intent_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        else:\n",
        "            raise ValueError('Invalid task_type. Use \"ner\" or \"intent\"')"
      ],
      "metadata": {
        "id": "afndjDPHAeHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiTaskBertWrapper(ner_model, intent_model)"
      ],
      "metadata": {
        "id": "tLfiD2uA50ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "5coZ3SS3_ryJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = list(model.ner_model.parameters()) + list(model.intent_model.parameters())\n",
        "optimizer = torch.optim.AdamW(params, lr=1e-5)\n",
        "\n",
        "ner_loss_function = torch.nn.CrossEntropyLoss()\n",
        "intent_loss_function = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "YSzomyoc_s7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        inputs = batch['text']\n",
        "        attention_masks = batch['attention_mask']\n",
        "        labels_ner = batch['entities']\n",
        "        labels_intent = batch['intent']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass for NER task\n",
        "        ner_outputs = model.ner_model(input_ids=inputs, attention_mask=attention_masks, labels=labels_ner, task_type='ner')\n",
        "        ner_loss = ner_loss_function(ner_outputs.logits.view(-1, num_ner_labels), labels_ner.view(-1))\n",
        "\n",
        "        ner_loss.backward()\n",
        "\n",
        "        # Forward pass for Intent Classification task\n",
        "        intent_outputs = model.intent_model(input_ids=inputs, attention_mask=attention_masks, labels=labels_intent, task_type='intent')\n",
        "        intent_loss = intent_loss_function(intent_outputs.logits.view(-1, num_intent_labels), labels_intent.view(-1))\n",
        "\n",
        "        intent_loss.backward()\n",
        "\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "-kwfmmiGAKtX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}