{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "lJDVuWO3B1pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CptExsPx7v1M"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForTokenClassification, BertForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import TextDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT tokenizer and models for token classification and sequence classification\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "ner_model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_ner_labels)\n",
        "intent_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_intent_labels)"
      ],
      "metadata": {
        "id": "SZaMCTlI71p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset = TextDataset(\n",
        "#     file_path = ,\n",
        "#     block_size = 128\n",
        "# )\n",
        "path = \"\"\n",
        "train_dataloader = DataLoader(path, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "DQt4_CYOBK6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam([\n",
        "    {'params': ner_model.parameters()},\n",
        "    {'params': intent_model.parameters()}\n",
        "], lr=1e-5)  # Define optimizer for both models\n",
        "\n",
        "num_epochs = 3  # Define number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:  # Iterate over your dataset batches\n",
        "        inputs = tokenizer(batch['text'], return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "        # Forward pass for NER\n",
        "        outputs_ner = ner_model(**inputs)\n",
        "        ner_labels = batch['ner_labels']\n",
        "        ner_loss = torch.nn.CrossEntropyLoss()(outputs_ner.logits.view(-1, num_ner_labels), ner_labels.view(-1))\n",
        "\n",
        "        # Forward pass for intent classification\n",
        "        outputs_intent = intent_model(**inputs)\n",
        "        intent_labels = batch['intent_labels']\n",
        "        intent_loss = torch.nn.CrossEntropyLoss()(outputs_intent.logits, intent_labels)\n",
        "\n",
        "        # Total loss (you might adjust weights for different tasks)\n",
        "        total_loss = ner_loss + intent_loss\n",
        "\n",
        "        # Backpropagation\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "h9JGFaBu8wGT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}