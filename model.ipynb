{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "lJDVuWO3B1pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CptExsPx7v1M"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForTokenClassification, BertForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import TextDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "id": "0xAaLdBZE0G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# Read the JSON file\n",
        "path = \"/content/drive/MyDrive/model/B_data.json\"\n",
        "with open(path, 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Assuming the dataset is a list of dictionaries\n",
        "\n",
        "# Define a custom Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Create an instance of CustomDataset\n",
        "custom_dataset = CustomDataset(dataset)\n",
        "\n",
        "# Create a DataLoader\n",
        "batch_size = 1\n",
        "train_dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "bXc4PNVFRP4D"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = []\n",
        "for sample in train_dataloader:\n",
        "    key.append(sample['entities'])"
      ],
      "metadata": {
        "id": "sMcD55taOmXE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities = []\n",
        "for substr in key:\n",
        "    tokens = substr[0].split()\n",
        "    entities.extend(tokens)"
      ],
      "metadata": {
        "id": "RuS23pIA8o0K"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities"
      ],
      "metadata": {
        "id": "KRXA6PeKCy4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = set(entities)\n",
        "print(key)\n",
        "print(len(key))\n",
        "num_ner_labels = len(key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJB6otmMQSDG",
        "outputId": "78a9f166-9a2c-4166-9a77-11f448d11ab3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'I-TASK', 'O', 'B-TIME', 'B-DATE', 'B-TASK', 'B-DUR', 'I-TIME', 'I-DUR', 'I-DATE'}\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = []\n",
        "for sample in train_dataloader:\n",
        "    l.extend(sample['intent'])"
      ],
      "metadata": {
        "id": "UY9epjj3RVri"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = set(l)\n",
        "print(l)\n",
        "print(len(l))\n",
        "num_intent_labels=len(l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAA0ljZPH0oL",
        "outputId": "41860064-6fc4-47a9-8889-9de79a3699e4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"'Set Timer'\", \"'Set Alarm'\", \"'Set Reminder'\", \"'Schedule Meeting'\", \"'Schedule Appointment'\"}\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT tokenizer and models for token classification and sequence classification\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "ner_model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_ner_labels)\n",
        "intent_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_intent_labels)"
      ],
      "metadata": {
        "id": "SZaMCTlI71p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing the dataset"
      ],
      "metadata": {
        "id": "-odMqRP2VICq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maybe I should try to set trincation and padding to False."
      ],
      "metadata": {
        "id": "n7tTgv64JFcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs = []\n",
        "\n",
        "max_sequence_length = 0  # Initialize maximum sequence length\n",
        "\n",
        "for batch in train_dataloader:\n",
        "    text = batch['text']\n",
        "    batch_tokenized = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "    input_ids = batch_tokenized['input_ids']\n",
        "    attention_mask = batch_tokenized['attention_mask']\n",
        "    tokenized_inputs.append((input_ids, attention_mask))\n",
        "\n",
        "    # Update maximum sequence length\n",
        "    max_sequence_length = max(max_sequence_length, input_ids.shape[1])\n",
        "\n",
        "# Pad sequences to the maximum sequence length\n",
        "for i in range(len(tokenized_inputs)):\n",
        "    input_ids, attention_mask = tokenized_inputs[i]\n",
        "    pad_length = max_sequence_length - input_ids.shape[1]\n",
        "    padded_input_ids = torch.nn.functional.pad(input_ids, (0, pad_length), value=tokenizer.pad_token_id)\n",
        "    padded_attention_mask = torch.nn.functional.pad(attention_mask, (0, pad_length), value=0)  # Assuming 0 for padding mask\n",
        "    tokenized_inputs[i] = (padded_input_ids, padded_attention_mask)\n",
        "\n",
        "# Concatenate input IDs and attention masks separately\n",
        "input_ids = torch.cat([tensor[0] for tensor in tokenized_inputs], dim=0)\n",
        "attention_mask = torch.cat([tensor[1] for tensor in tokenized_inputs], dim=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "DGFDqLQTVHcL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids"
      ],
      "metadata": {
        "id": "VpCPdPOoJAqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# task -> B-TASK\n",
        "# date -> B-DATE\n",
        "# duration -> B-DUR\n",
        "# days -> B_DAY\n",
        "# time -> B-TIME\n",
        "# specific_time\n",
        "encoded_ner_labels = []\n",
        "label_ner_map = {'O': 0, 'B-DATE': 1, 'I-DATE': 2, 'B-TIME': 3, 'I-TIME': 4, 'B-TASK': 5, 'I-TASK': 6, 'B-DUR': 7, 'I-DUR': 8}\n",
        "for sample in train_dataloader:\n",
        "    encoded_ner_labels.extend([label_ner_map[label] for label in sent_label.split()] for sent_label in sample['entities'])\n",
        "encoded_ner_labels"
      ],
      "metadata": {
        "id": "Y6J-hyyTpSAD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c46f5e5b-899e-4cd7-e247-2e4c0aae7331"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 7, 8],\n",
              " [0, 0, 0, 0, 0, 0, 3, 4, 1],\n",
              " [0, 0, 0, 0, 1, 2, 0, 3, 4],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 3, 4],\n",
              " [0, 0, 0, 0, 3, 4],\n",
              " [0, 0, 0, 5, 6, 0, 7, 8],\n",
              " [0, 0, 0, 0, 1, 2, 3],\n",
              " [0, 0, 0, 0, 0, 0, 5, 0, 7, 8],\n",
              " [0, 0, 0, 0, 5, 6, 0, 3, 4, 0, 1],\n",
              " [0, 0, 0, 0, 0, 1, 2, 0, 3, 4],\n",
              " [0, 0, 0, 0, 0, 3, 5],\n",
              " [0, 0, 0, 5, 6, 1, 3],\n",
              " [0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0, 3, 4]]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_dataloader:\n",
        "    print(i['entities'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db3lumxzUty7",
        "outputId": "a792ca63-7bf0-4e61-9fd7-3852730956fe"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O O O O B-DUR I-DUR']\n",
            "['O O O O O O B-TIME I-TIME B-DATE']\n",
            "['O O O O B-DATE I-DATE O B-TIME I-TIME']\n",
            "['O O O O O O O O O O B-DATE']\n",
            "['O O O O O O O O B-DATE I-DATE I-DATE I-DATE O B-TIME I-TIME']\n",
            "['O O O O B-TIME I-TIME']\n",
            "['O O O B-TASK I-TASK O B-DUR I-DUR']\n",
            "['O O O O B-DATE I-DATE B-TIME']\n",
            "['O O O O O O B-TASK O B-DUR I-DUR']\n",
            "['O O O O B-TASK I-TASK O B-TIME I-TIME O B-DATE']\n",
            "['O O O O O B-DATE I-DATE O B-TIME I-TIME']\n",
            "['O O O O O B-TIME B-TASK']\n",
            "['O O O B-TASK I-TASK B-DATE B-TIME']\n",
            "['O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O B-TIME I-TIME']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "sample_entities = []\n",
        "for sample in train_dataloader:\n",
        "    sample_entities.extend(sample['entities'])\n",
        "\n",
        "# Flatten the list of strings into a single list of tokens\n",
        "all_entities = [entity for sample in sample_entities for entity in sample.split()]\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit LabelEncoder to all unique entity labels\n",
        "label_encoder.fit(all_entities)\n",
        "\n",
        "# Encode entities for each sample in sample_entities\n",
        "encoded_entities = [\n",
        "    label_encoder.transform(entity.split()).tolist() for entity in sample_entities\n",
        "]\n",
        "\n",
        "print(encoded_entities)\n"
      ],
      "metadata": {
        "id": "TdU2s6l0pUHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6ba95c0-4067-4a80-beb8-f273fdd8a657"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[8, 8, 8, 8, 1, 5], [8, 8, 8, 8, 8, 8, 3, 7, 0], [8, 8, 8, 8, 0, 4, 8, 3, 7], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0], [8, 8, 8, 8, 8, 8, 8, 8, 0, 4, 4, 4, 8, 3, 7], [8, 8, 8, 8, 3, 7], [8, 8, 8, 2, 6, 8, 1, 5], [8, 8, 8, 8, 0, 4, 3], [8, 8, 8, 8, 8, 8, 2, 8, 1, 5], [8, 8, 8, 8, 2, 6, 8, 3, 7, 8, 0], [8, 8, 8, 8, 8, 0, 4, 8, 3, 7], [8, 8, 8, 8, 8, 3, 2], [8, 8, 8, 2, 6, 0, 3], [8, 8, 8, 8, 8, 8, 0, 4, 4, 4, 4, 8, 3, 7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intent_labels = list(l)\n",
        "\n",
        "encoded_intent_labels = label_encoder.fit_transform(intent_labels)"
      ],
      "metadata": {
        "id": "_fSSwvVtI1MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenizedDataset(Dataset):\n",
        "    def __init__(self, tokenized_inputs, encoded_ner_labels, encoded_intent_labels):\n",
        "        self.tokenized_inputs = tokenized_inputs\n",
        "        self.encoded_ner_labels = encoded_ner_labels\n",
        "        self.encoded_intent_labels = encoded_intent_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inputs = self.tokenized_inputs[idx]\n",
        "        ner_labels = self.encoded_ner_labels[idx]\n",
        "        intent_labels = self.encoded_intent_labels[idx]\n",
        "\n",
        "        return {\n",
        "            'inputs': inputs,\n",
        "            'ner_labels': ner_labels,\n",
        "            'intent_labels': intent_labels\n",
        "        }"
      ],
      "metadata": {
        "id": "LqdVj_pZo6P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cust_dataset = CustomDataset(tokenized_inputs, encoded_ner_labels, encoded_intent_labels)\n",
        "data_loader = DataLoader(cust_dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "FSIViFHI1b48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TafoSlh3S8Bn",
        "outputId": "99626944-db75-4499-c23b-708e8806fbfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intent_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6niOzBtQS_Am",
        "outputId": "aace6bb3-584e-4a82-b0b1-43d9d6125bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in train_dataloader:\n",
        "    print(sample['entities'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO7vbAMi0R7H",
        "outputId": "48e83237-18db-40d6-bc1b-9e0c36ffe9ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'task': ['cooking'], 'duration': ['1 hour']}\n",
            "{'time': ['3 PM'], 'date': ['tomorrow']}\n",
            "{'duration': ['10 minutes']}\n",
            "{'task': ['5-minute meditation session'], 'duration': ['5 minutes']}\n",
            "{'task': ['webinar'], 'time': ['2 PM'], 'duration': ['2 days']}\n",
            "{'date': ['10th of next month'], 'time': ['2 PM']}\n",
            "{'task': ['20-minute workout session'], 'duration': ['20 minutes']}\n",
            "{'task': ['buy groceries'], 'date': ['Saturday'], 'time': ['afternoon']}\n",
            "{'task': ['study session'], 'duration': ['45 minutes']}\n",
            "{'time': ['7:30 AM'], 'date': ['tomorrow']}\n",
            "{'task': ['water the plants'], 'time': ['9 AM'], 'days': [('Tuesday',), ('Thursday',)]}\n",
            "{'task': ['pay bills'], 'date': ['last day of the month']}\n",
            "{'duration': ['30 minutes'], 'task': ['call John']}\n",
            "{'task': ['buy groceries'], 'date': ['tomorrow'], 'time': ['morning']}\n",
            "{'date': ['end of the month'], 'time': ['4:30 PM']}\n",
            "{'date': ['next Monday'], 'time': ['morning'], 'specific_time': ['10:30']}\n",
            "{'date': ['March 20th'], 'time': ['10:30 AM']}\n",
            "{'time': ['2:30 PM'], 'date': ['15th of this month']}\n",
            "{'task': ['15-minute break'], 'duration': ['15 minutes']}\n",
            "{'task': ['send the report'], 'time': ['4:30 PM'], 'date': ['today']}\n",
            "{'task': ['call Sarah'], 'date': ['next Wednesday'], 'time': ['afternoon']}\n",
            "{'task': ['send the report'], 'time': ['9 AM'], 'date': ['tomorrow']}\n",
            "{'task': ['pick up the laundry'], 'time': ['afternoon'], 'days': [('Friday',)]}\n",
            "{'time': ['9 AM'], 'date': ['next Friday']}\n",
            "{'date': ['next Friday'], 'time': ['noon']}\n",
            "{'task': ['project deadline'], 'time': ['5 PM'], 'date': ['Friday']}\n",
            "{'date': ['April 5th'], 'time': ['morning'], 'specific_time': ['11:00']}\n",
            "{'date': ['next Wednesday'], 'time': ['afternoon']}\n",
            "{'task': ['10-minute break'], 'duration': ['10 minutes']}\n",
            "{'task': ['presentation'], 'time': ['4 PM'], 'duration': ['3 hours'], 'date': ['today']}\n",
            "{'time': ['7 AM']}\n",
            "{'date': ['Monday']}\n",
            "{'date': ['May 15th'], 'time': ['evening']}\n",
            "{'time': ['6:45 AM']}\n",
            "{'date': ['first Monday of next month'], 'time': ['3 PM']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam([\n",
        "    {'params': ner_model.parameters()},\n",
        "    {'params': intent_model.parameters()}\n",
        "], lr=1e-5)  # Define optimizer for both models\n",
        "\n",
        "num_epochs = 3  # Define number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:  # Iterate over your dataset batches\n",
        "        inputs = tokenizer(batch['text'], return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "        # Forward pass for intent classification\n",
        "        outputs_intent = intent_model(**inputs)\n",
        "        intent_labels = batch['intent']\n",
        "        intent_loss = torch.nn.CrossEntropyLoss()(outputs_intent.logits, intent_labels)\n",
        "\n",
        "        # Forward pass for NER\n",
        "        outputs_ner = ner_model(**inputs)\n",
        "        ner_labels = batch['entities']\n",
        "        ner_loss = torch.nn.CrossEntropyLoss()(outputs_ner.logits.view(-1, num_ner_labels), ner_labels.view(-1))\n",
        "\n",
        "        # Total loss (you might adjust weights for different tasks)\n",
        "        total_loss = ner_loss + intent_loss\n",
        "\n",
        "        # Backpropagation\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "h9JGFaBu8wGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam([\n",
        "    {'params': ner_model.parameters()},\n",
        "    {'params': intent_model.parameters()}\n",
        "], lr=1e-5)  # Define optimizer for both models\n",
        "\n",
        "num_epochs = 3  # Define number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:  # Iterate over your dataset batches\n",
        "        inputs = tokenizer(batch['text'], return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "        # Forward pass for intent classification\n",
        "        outputs_intent = intent_model(**inputs)\n",
        "        intent_labels = batch['intent']\n",
        "        intent_loss = torch.nn.CrossEntropyLoss()(outputs_intent.logits, intent_labels)\n",
        "\n",
        "        # Forward pass for NER\n",
        "        outputs_ner = ner_model(**inputs)\n",
        "        ner_labels = batch['entities']\n",
        "        ner_loss = torch.nn.CrossEntropyLoss()(outputs_ner.logits.view(-1, num_ner_labels), ner_labels.view(-1))\n",
        "\n",
        "        # Total loss (you might adjust weights for different tasks)\n",
        "        total_loss = ner_loss + intent_loss\n",
        "\n",
        "        # Backpropagation\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        # No idea yet"
      ],
      "metadata": {
        "id": "MZBdetDs9xfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the Multi-Task Architecture"
      ],
      "metadata": {
        "id": "OZ5Jw2arAaVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskBertWrapper(nn.Module):\n",
        "    def __init__(self, ner_model, intent_model):\n",
        "        super().__init__()\n",
        "        self.ner_model = ner_model\n",
        "        self.intent_model = intent_model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None, task_type=None):\n",
        "        if task_type = 'ner':\n",
        "            self.ner_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        elif task_type = 'intent':\n",
        "            self.intent_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        else:\n",
        "            raise ValueError('Invalid task_type. Use \"ner\" or \"intent\"')"
      ],
      "metadata": {
        "id": "afndjDPHAeHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiTaskBertWrapper(ner_model, intent_model)"
      ],
      "metadata": {
        "id": "tLfiD2uA50ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "5coZ3SS3_ryJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = list(model.ner_model.parameters()) + list(model.intent_model.parameters())\n",
        "optimizer = torch.optim.AdamW(params, lr=1e-5)\n",
        "\n",
        "ner_loss_function = torch.nn.CrossEntropyLoss()\n",
        "intent_loss_function = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "YSzomyoc_s7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        inputs = batch['text']\n",
        "        attention_masks = batch['attention_mask']\n",
        "        labels_ner = batch['entities']\n",
        "        labels_intent = batch['intent']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass for NER task\n",
        "        ner_outputs = model.ner_model(input_ids=inputs, attention_mask=attention_masks, labels=labels_ner, task_type='ner')\n",
        "        ner_loss = ner_loss_function(ner_outputs.logits.view(-1, num_ner_labels), labels_ner.view(-1))\n",
        "\n",
        "        ner_loss.backward()\n",
        "\n",
        "        # Forward pass for Intent Classification task\n",
        "        intent_outputs = model.intent_model(input_ids=inputs, attention_mask=attention_masks, labels=labels_intent, task_type='intent')\n",
        "        intent_loss = intent_loss_function(intent_outputs.logits.view(-1, num_intent_labels), labels_intent.view(-1))\n",
        "\n",
        "        intent_loss.backward()\n",
        "\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "-kwfmmiGAKtX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}