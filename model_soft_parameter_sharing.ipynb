{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "8b9zbC6ABPBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb2bc80c-e00d-48e9-a24b-b3d21b0264df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: seqeval[gpu] in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval[gpu]) (1.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece] seqeval[gpu]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "jnGRzmhqBWF1"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast, BertForTokenClassification, BertForSequenceClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import TextDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "X9ZtmdcxBYMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4220f85f-9936-4835-c290-daad8c310267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "vxqkLzucfiG_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# Read the JSON file\n",
        "path = \"/content/drive/MyDrive/model/B_data.json\"\n",
        "with open(path, 'r') as f:\n",
        "    dataset = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/model/training_set.json\"\n",
        "with open(path, 'r') as f:\n",
        "    test_dataset = json.load(f)"
      ],
      "metadata": {
        "id": "_ZTqlfb_qrBo"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "JQin9IoNfnh-"
      },
      "outputs": [],
      "source": [
        "text, intent, ner = [], [], []\n",
        "for i in dataset:\n",
        "    text.append(i['text'])\n",
        "    intent.append(i['intent'])\n",
        "    ner.append(i['entities'].split())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text, test_intent, test_ner = [], [], []\n",
        "for i in test_dataset:\n",
        "    test_text.append(i['text'])\n",
        "    test_intent.append(i['intent'])\n",
        "    test_ner.append(i['entities'].split())"
      ],
      "metadata": {
        "id": "j8zDpSSQq1KY"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just test"
      ],
      "metadata": {
        "id": "LU2rkg73rrvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_ner[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3RdWsCKtzx9",
        "outputId": "406e1067-0730-465e-f73c-874fbef0a1ad"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'B-TASK',\n",
              " 'I-TASK',\n",
              " 'B-DATE',\n",
              " 'I-DATE',\n",
              " 'O',\n",
              " 'B-TIME',\n",
              " 'I-TIME']"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_ = set(test_intent)\n",
        "num_i = len(unique_)\n",
        "\n",
        "unique_, num_i"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8YtM-6frklA",
        "outputId": "ecf51e30-b38e-4d1c-ccf5-bd555a9f1edb"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({\"'Schedule Appointment'\",\n",
              "  \"'Schedule Meeting'\",\n",
              "  \"'Set Alarm'\",\n",
              "  \"'Set Reminder'\",\n",
              "  \"'Set Timer'\"},\n",
              " 5)"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_r = [tag for subset in test_ner for tag in subset]\n",
        "uni = set(one_r)\n",
        "num_ = len(uni)\n",
        "uni, num_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ4AXcsfrvsm",
        "outputId": "61a296fc-4c35-4a1b-971b-0ac6250931bf"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'B-DATE',\n",
              "  'B-DUR',\n",
              "  'B-TASK',\n",
              "  'B-TIME',\n",
              "  'I-DATE',\n",
              "  'I-DUR',\n",
              "  'I-TASK',\n",
              "  'I-TIME',\n",
              "  'O'},\n",
              " 9)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delete what is above"
      ],
      "metadata": {
        "id": "TbImDj_wr6iN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTUJWv92f3sx",
        "outputId": "f945847f-814a-487f-8a65-b5fffb47793b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({\"'Schedule Appointment'\",\n",
              "  \"'Schedule Meeting'\",\n",
              "  \"'Set Alarm'\",\n",
              "  \"'Set Reminder'\",\n",
              "  \"'Set Timer'\"},\n",
              " 5)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "source": [
        "unique_intents = set(intent)\n",
        "num_intent_labels = len(unique_intents)\n",
        "\n",
        "unique_intents, num_intent_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdoGreyqgosK",
        "outputId": "d9536df4-25e7-462d-f3fa-8361e6d3b3d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'B-DATE',\n",
              "  'B-DUR',\n",
              "  'B-TASK',\n",
              "  'B-TIME',\n",
              "  'I-DATE',\n",
              "  'I-DUR',\n",
              "  'I-TASK',\n",
              "  'I-TIME',\n",
              "  'O'},\n",
              " 9)"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "one_dimensional_ner = [tag for subset in ner for tag in subset ]\n",
        "unique_ner = set(one_dimensional_ner)\n",
        "num_ner_labels = len(unique_ner)\n",
        "unique_ner, num_ner_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "KDSp3FM-gtCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8c09606-75c7-4756-b32d-18672d839d5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "ner_model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_ner_labels)\n",
        "intent_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_intent_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "gIzCQacBpFzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a786d7-4685-4c90-91cb-704696fd0c51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'O',\n",
              " 1: 'B-DATE',\n",
              " 2: 'I-DATE',\n",
              " 3: 'B-TIME',\n",
              " 4: 'I-TIME',\n",
              " 5: 'B-TASK',\n",
              " 6: 'I-TASK',\n",
              " 7: 'B-DUR',\n",
              " 8: 'I-DUR'}"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "labels_to_ids_ner = {\n",
        "    'O': 0,\n",
        "    'B-DATE': 1,\n",
        "    'I-DATE': 2,\n",
        "    'B-TIME': 3,\n",
        "    'I-TIME': 4,\n",
        "    'B-TASK': 5,\n",
        "    'I-TASK': 6,\n",
        "    'B-DUR': 7,\n",
        "    'I-DUR': 8\n",
        "    }\n",
        "\n",
        "ids_to_labels_ner = {v: k for k, v in labels_to_ids_ner.items()}\n",
        "ids_to_labels_ner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "t3y8NTdexh0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93b6ae5f-d8c4-4ca5-e867-c2a43a21833b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: \"'Schedule Appointment'\",\n",
              " 1: \"'Schedule Meeting'\",\n",
              " 2: \"'Set Alarm'\",\n",
              " 3: \"'Set Reminder'\",\n",
              " 4: \"'Set Timer'\"}"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ],
      "source": [
        "labels_to_ids_intent = {\n",
        "    \"'Schedule Appointment'\": 0,\n",
        "    \"'Schedule Meeting'\": 1,\n",
        "    \"'Set Alarm'\": 2,\n",
        "    \"'Set Reminder'\": 3,\n",
        "    \"'Set Timer'\": 4\n",
        "}\n",
        "\n",
        "ids_to_labels_intent = {v: k for k, v in labels_to_ids_intent.items()}\n",
        "ids_to_labels_intent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "QvkVdQRVjuLM"
      },
      "outputs": [],
      "source": [
        "class dataset(Dataset):\n",
        "    def __init__(self, text, intent, ner, tokenizer, max_len=128):\n",
        "        self.len = len(text)\n",
        "        self.text = text\n",
        "        self.intent = intent\n",
        "        self.ner = ner\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # step 1: get the sentence, ner label, and intent_label\n",
        "        sentence = self.text[index].strip()\n",
        "        intent_label = self.intent[index].strip()\n",
        "        ner_labels = self.ner[index]\n",
        "\n",
        "        # step 2: use tokenizer to encode a sentence (includes padding/truncation up to max length)\n",
        "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" which highlights where each token starts and ends\n",
        "        encoding = self.tokenizer(\n",
        "            sentence,\n",
        "            return_offsets_mapping=True,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_len\n",
        "        )\n",
        "\n",
        "        # step 3: create ner token labels only for first word pieces of each tokenized word\n",
        "        tokenized_ner_labels = [labels_to_ids_ner[label] for label in ner_labels]\n",
        "        # create an empty array of -100 of length max_length\n",
        "        encoded_ner_labels = np.ones(len(encoding['offset_mapping']), dtype=int) * -100\n",
        "\n",
        "        # set only labels whose first offset position is 0 and the second is not 0\n",
        "        i = 0\n",
        "        prev = -1\n",
        "        for idx, mapping in enumerate(encoding['offset_mapping']):\n",
        "            if mapping[0] == mapping[1] == 0:\n",
        "                continue\n",
        "            if mapping[0] != prev:\n",
        "                # overwrite label\n",
        "                encoded_ner_labels[idx] = tokenized_ner_labels[i]\n",
        "                prev = mapping[1]\n",
        "                i += 1\n",
        "            else:\n",
        "                prev = mapping[1]\n",
        "\n",
        "        # create intent token labels\n",
        "        tokenized_intent_label = labels_to_ids_intent[intent_label]\n",
        "\n",
        "        # step 4: turn everything into Pytorch tensors\n",
        "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
        "        item['ner_labels'] = torch.as_tensor(encoded_ner_labels)\n",
        "        item['intent_labels'] = torch.as_tensor(tokenized_intent_label)\n",
        "\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "cEaNyuELzNWa"
      },
      "outputs": [],
      "source": [
        "training_set = dataset(text, intent, ner, tokenizer)\n",
        "test_set = dataset(test_text, test_intent, test_ner, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "6gqvwYgr0jfS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc512b6d-a332-4bbc-fa93-56ba0f84d404"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([ 101, 2275, 2019, 8598, 2005, 1021, 2572, 1012,  102,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
              " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'offset_mapping': tensor([[ 0,  0],\n",
              "         [ 0,  3],\n",
              "         [ 4,  6],\n",
              "         [ 7, 12],\n",
              "         [13, 16],\n",
              "         [17, 18],\n",
              "         [19, 21],\n",
              "         [21, 22],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0]]),\n",
              " 'ner_labels': tensor([-100,    0,    0,    0,    0,    3,    4, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100]),\n",
              " 'intent_labels': tensor(2)}"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ],
      "source": [
        "training_set[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbBNIl17mTVH"
      },
      "source": [
        "Let us verify that the input ids and corresponding targets are correct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "5Tf1jeZb0k-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e09c5b0-63ee-41b1-ccc9-ae4c4c6593a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] -- -100\n",
            "schedule -- 0\n",
            "a -- 0\n",
            "dentist -- 5\n",
            "appointment -- 6\n",
            "for -- 0\n",
            "april -- 1\n",
            "5th -- 2\n",
            "at -- 0\n",
            "11 -- 3\n",
            ": -- -100\n",
            "00 -- -100\n",
            "in -- 4\n",
            "the -- 4\n",
            "morning -- 4\n",
            ". -- -100\n",
            "[SEP] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n"
          ]
        }
      ],
      "source": [
        "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[20]['input_ids']), training_set[20]['ner_labels']):\n",
        "    print(f\"{token} -- {label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "k8gK0rcxnQgi"
      },
      "outputs": [],
      "source": [
        "# The dataset is small, batch_size of 1 would not impact the training time significantly\n",
        "training_loader = DataLoader(training_set, batch_size=1)\n",
        "test_loader = DataLoader(test_set, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "tjIswaA6ojMQ"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "xy1OOPFKZ4fs",
        "outputId": "50f4dc15-a813-4402-f99f-52a3b1558c84"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ner_model.to(device)\n",
        "intent_model.to(device)"
      ],
      "metadata": {
        "id": "i_S3mmSta5T0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "598a625a-0873-4e9e-bf2e-ae77f93c9080"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf352WfopY0y"
      },
      "source": [
        "The initial loss of the model should be close to -ln(1/num_labels)=-ln(1/9). In this case it is 2.20.\n",
        "Why? Because we are using cross entropy loss. The cross entropy loss is defined as -ln(probability score of the model for the correct class). In the beginning, the weights are random, so the probability distribution for all of the classes for a given token will be uniform, meaning that the probability for the correct class will be near 1/9. The loss for a given token will thus be -ln(1/9). As PyTorch's CrossEntropyLoss (which is used by BertForTokenClassification) uses mean reduction by default, it will compute the mean loss for each of the tokens in the sequence for which a label is provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jjk1I1coLLI",
        "outputId": "63b2dfe8-06d5-4c86-9bfa-3d645b77d495"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.0793, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ],
      "source": [
        "inputs = training_set[2]\n",
        "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
        "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
        "labels = inputs[\"ner_labels\"].unsqueeze(0)\n",
        "\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "outputs = ner_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "initial_loss = outputs[0]\n",
        "initial_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iJ_clHk6ldm"
      },
      "source": [
        "The shape of logits must be __[batch_size, sequence_length, num_labels]__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "619BrUfmqBKS",
        "outputId": "b728541e-9480-40ac-b1fe-98289e28a4e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ],
      "source": [
        "tr_logits = outputs[1]\n",
        "tr_logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDZfSrT-7RdO"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam([\n",
        "    {'params': ner_model.parameters()},\n",
        "    {'params': intent_model.parameters()}\n",
        "], lr=1e-5)"
      ],
      "metadata": {
        "id": "0Jfa3o8tUfLU"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi Task Learning Architecture"
      ],
      "metadata": {
        "id": "olgG_5XNtDga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Soft Parameter Sharing"
      ],
      "metadata": {
        "id": "Es-sp93awEU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftParameterSharing(torch.nn.Module):\n",
        "    def __init__(self, ner_model, intent_model, alpha):\n",
        "        super(SoftParameterSharing, self).__init__()\n",
        "        self.ner_model = ner_model\n",
        "        self.intent_model = intent_model\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, input_ids, mask, ner_labels, intent_labels):\n",
        "        ner_logits = self.ner_model(input_ids=input_ids, attention_mask=mask, labels=ner_labels)\n",
        "\n",
        "        intent_logits = self.intent_model(input_ids=input_ids, attention_mask=mask, labels=intent_labels)\n",
        "\n",
        "        return ner_logits, intent_logits\n",
        "\n",
        "    def soft_penalty(self):\n",
        "        a = 0\n",
        "        penalty = 0.0\n",
        "        for param_ner, param_intent in zip(self.ner_model.parameters(), self.intent_model.parameters()):\n",
        "            if param_ner.shape != param_intent.shape:\n",
        "                continue\n",
        "\n",
        "            penalty += torch.norm(param_ner - param_intent, p=2) ** 2\n",
        "\n",
        "        return self.alpha * penalty\n"
      ],
      "metadata": {
        "id": "CMOzHXE4tHzb"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SoftParameterSharing(ner_model, intent_model, 0.05)"
      ],
      "metadata": {
        "id": "KmJTNuQQDTrS"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lC_e7bY5dqS",
        "outputId": "c4a0aebe-255f-4088-8077-53a1634def6b"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SoftParameterSharing(\n",
              "  (ner_model): BertForTokenClassification(\n",
              "    (bert): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-11): 12 x BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
              "  )\n",
              "  (intent_model): BertForSequenceClassification(\n",
              "    (bert): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-11): 12 x BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (pooler): BertPooler(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (activation): Tanh()\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    tr_ner_loss, tr_ner_accuracy = 0, 0\n",
        "    tr_intent_loss, tr_intent_accuracy = 0, 0\n",
        "    nb_tr_steps = 0\n",
        "    tr_ner_preds, tr_ner_labels = [], []\n",
        "    tr_intent_labels, tr_intent_predictions = [], []\n",
        "    model.train()\n",
        "\n",
        "\n",
        "    for idx, batch in enumerate(training_loader):\n",
        "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "        ner_labels = batch['ner_labels'].to(device, dtype=torch.long)\n",
        "        intent_labels = batch['intent_labels'].to(device, dtype=torch.long)\n",
        "\n",
        "        ner_logits, intent_logits = model(ids, mask, ner_labels, intent_labels)\n",
        "\n",
        "        ner_loss = ner_logits.loss\n",
        "        intent_loss = intent_logits.loss\n",
        "\n",
        "        soft_penalty = model.soft_penalty()\n",
        "\n",
        "        comb_loss = ner_loss + intent_loss + soft_penalty\n",
        "        # till here\n",
        "\n",
        "        tr_ner_loss += ner_logits['loss']\n",
        "        tr_intent_loss += intent_logits['loss']\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "        if idx % 5 == 0:\n",
        "            loss_ner_step = tr_ner_loss / nb_tr_steps\n",
        "            loss_intent_step = tr_intent_loss / nb_tr_steps\n",
        "            print(f\"Training NER loss per {idx} training steps: {loss_ner_step}\")\n",
        "            print(f\"Training INTENT loss per {idx} training steps: {loss_intent_step}\")\n",
        "\n",
        "        # compute training accuracy (FOR NER)\n",
        "        flattened_ner_targets = ner_labels.view(-1) # shape (batch_size * seq_len)\n",
        "        active_ner_logits = ner_logits.logits.view(-1, num_ner_labels) # shape (batch_size*seq_len, num_labels)\n",
        "        flattened_ner_predictions = torch.argmax(active_ner_logits, axis=1) # shape (batch_size * seq_len)\n",
        "\n",
        "        # compute accuracy only at active labels\n",
        "        active_ner_accuracy = ner_labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        ac_ner_labels = torch.masked_select(flattened_ner_targets, active_ner_accuracy)\n",
        "        ner_predictions = torch.masked_select(flattened_ner_predictions, active_ner_accuracy)\n",
        "\n",
        "        tr_ner_labels.extend(ac_ner_labels)\n",
        "        tr_ner_preds.extend(ner_predictions)\n",
        "\n",
        "        # compute accuracy for intent_model\n",
        "        # I CAN MAKE THE CALCULATION MUCH EASIER\n",
        "        # FIGURE IT OUT\n",
        "        flattened_intent_targets = intent_labels.view(-1)\n",
        "        active_intent_logits = intent_logits.logits.view(-1, num_intent_labels)\n",
        "        flattened_intent_predictions = torch.argmax(active_intent_logits, axis=1)\n",
        "\n",
        "        sample_intent_accuracy = intent_labels.view(-1)\n",
        "        active_intent_accuracy = torch.ones_like(sample_intent_accuracy, dtype=torch.bool)\n",
        "\n",
        "        ac_intent_labels = torch.masked_select(flattened_intent_targets, active_intent_accuracy)\n",
        "        intent_predictions = torch.masked_select(flattened_intent_predictions, active_intent_accuracy)\n",
        "\n",
        "        tr_intent_labels.extend(ac_intent_labels)\n",
        "        tr_intent_predictions.extend(intent_predictions)\n",
        "\n",
        "        tmp_tr_ner_accuracy = accuracy_score(ac_ner_labels.cpu().numpy(), ner_predictions.cpu().numpy())\n",
        "        tmp_tr_intent_accuracy = accuracy_score(ac_intent_labels.cpu().numpy(), intent_predictions.cpu().numpy())\n",
        "\n",
        "\n",
        "        tr_ner_accuracy += tmp_tr_ner_accuracy\n",
        "        tr_intent_accuracy += tmp_tr_intent_accuracy\n",
        "\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters = model.parameters(), max_norm=10\n",
        "        )\n",
        "\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        comb_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = (tr_ner_loss + tr_intent_loss) / nb_tr_steps\n",
        "    tr_ner_accuracy = tr_ner_accuracy / nb_tr_steps\n",
        "    tr_intent_accuracy = tr_intent_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training NER accuracy epoch: {tr_ner_accuracy}\")\n",
        "    print(f\"Training INTENT accuracy epoch: {tr_intent_accuracy}\")\n"
      ],
      "metadata": {
        "id": "3UM6yOVR4g9_"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3):\n",
        "    print(f\"Training epoch: {epoch+1}\")\n",
        "    print(\"----------------------------\")\n",
        "    train(epoch)"
      ],
      "metadata": {
        "id": "JHz3SgpIfsxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58185ff8-91d5-4caa-dcf8-48faaf0e1599"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch: 1\n",
            "----------------------------\n",
            "Training NER loss per 0 training steps: 2.0993802547454834\n",
            "Training INTENT loss per 0 training steps: 1.991005778312683\n",
            "Training NER loss per 5 training steps: 1.9092364311218262\n",
            "Training INTENT loss per 5 training steps: 1.6928666830062866\n",
            "Training NER loss per 10 training steps: 1.8140372037887573\n",
            "Training INTENT loss per 10 training steps: 1.6224908828735352\n",
            "Training NER loss per 15 training steps: 1.7497143745422363\n",
            "Training INTENT loss per 15 training steps: 1.5813772678375244\n",
            "Training NER loss per 20 training steps: 1.7041735649108887\n",
            "Training INTENT loss per 20 training steps: 1.5426836013793945\n",
            "Training NER loss per 25 training steps: 1.6801238059997559\n",
            "Training INTENT loss per 25 training steps: 1.526710033416748\n",
            "Training NER loss per 30 training steps: 1.6350517272949219\n",
            "Training INTENT loss per 30 training steps: 1.5193252563476562\n",
            "Training loss epoch: 3.118429183959961\n",
            "Training NER accuracy epoch: 0.5190478569049998\n",
            "Training INTENT accuracy epoch: 0.42857142857142855\n",
            "Training epoch: 2\n",
            "----------------------------\n",
            "Training NER loss per 0 training steps: 1.0433286428451538\n",
            "Training INTENT loss per 0 training steps: 1.5934861898422241\n",
            "Training NER loss per 5 training steps: 0.8800166845321655\n",
            "Training INTENT loss per 5 training steps: 1.45876145362854\n",
            "Training NER loss per 10 training steps: 0.9856969714164734\n",
            "Training INTENT loss per 10 training steps: 1.3676377534866333\n",
            "Training NER loss per 15 training steps: 1.0152839422225952\n",
            "Training INTENT loss per 15 training steps: 1.329488754272461\n",
            "Training NER loss per 20 training steps: 1.0344889163970947\n",
            "Training INTENT loss per 20 training steps: 1.332230806350708\n",
            "Training NER loss per 25 training steps: 1.0634366273880005\n",
            "Training INTENT loss per 25 training steps: 1.3087726831436157\n",
            "Training NER loss per 30 training steps: 1.0611546039581299\n",
            "Training INTENT loss per 30 training steps: 1.2993745803833008\n",
            "Training loss epoch: 2.3379390239715576\n",
            "Training NER accuracy epoch: 0.6742661306947023\n",
            "Training INTENT accuracy epoch: 0.5428571428571428\n",
            "Training epoch: 3\n",
            "----------------------------\n",
            "Training NER loss per 0 training steps: 0.7988550662994385\n",
            "Training INTENT loss per 0 training steps: 1.396341323852539\n",
            "Training NER loss per 5 training steps: 0.5523965954780579\n",
            "Training INTENT loss per 5 training steps: 1.26063871383667\n",
            "Training NER loss per 10 training steps: 0.6664556860923767\n",
            "Training INTENT loss per 10 training steps: 1.2162548303604126\n",
            "Training NER loss per 15 training steps: 0.6926999092102051\n",
            "Training INTENT loss per 15 training steps: 1.187105655670166\n",
            "Training NER loss per 20 training steps: 0.7227411270141602\n",
            "Training INTENT loss per 20 training steps: 1.2084739208221436\n",
            "Training NER loss per 25 training steps: 0.7521076202392578\n",
            "Training INTENT loss per 25 training steps: 1.1567047834396362\n",
            "Training NER loss per 30 training steps: 0.7529919743537903\n",
            "Training INTENT loss per 30 training steps: 1.145678162574768\n",
            "Training loss epoch: 1.8597174882888794\n",
            "Training NER accuracy epoch: 0.8033778919493204\n",
            "Training INTENT accuracy epoch: 0.6571428571428571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the model"
      ],
      "metadata": {
        "id": "GIBPDraRgRps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(ner_model, intent_model, test_loader):\n",
        "    ner_model.eval()\n",
        "    intent_model.eval()\n",
        "\n",
        "    eval_ner_loss, eval_ner_accuracy = 0, 0\n",
        "    eval_intent_loss, eval_intent_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_ner_preds, eval_ner_labels = [], []\n",
        "    eval_intent_preds, eval_intent_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(test_loader):\n",
        "            ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "            ner_labels = batch['ner_labels'].to(device, dtype=torch.long)\n",
        "            intent_labels = batch['intent_labels'].to(device, dtype=torch.long)\n",
        "\n",
        "            ner_logits = ner_model(input_ids=ids, attention_mask=mask, labels=ner_labels)\n",
        "            intent_logits = intent_model(input_ids=ids, attention_mask=mask, labels=intent_labels)\n",
        "\n",
        "            eval_ner_loss += ner_logits['loss']\n",
        "            eval_intent_loss += intent_logits['loss']\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "            if idx % 5 == 0:\n",
        "                loss_ner_step = eval_ner_loss / nb_eval_steps\n",
        "                loss_intent_step = eval_intent_loss / nb_eval_steps\n",
        "                print(f\"Validation NER loss per {idx} training steps: {loss_ner_step}\")\n",
        "                print(f\"Validation INTENT loss per {idx} training steps: {loss_intent_step}\")\n",
        "\n",
        "            # compute training accuracy (FOR NER)\n",
        "            flattened_ner_targets = ner_labels.view(-1) # shape (batch_size * seq_len)\n",
        "            active_ner_logits = ner_logits.logits.view(-1, ner_model.num_labels) # shape (batch_size*seq_len, num_labels)\n",
        "            flattened_ner_predictions = torch.argmax(active_ner_logits, axis=1) # shape (batch_size * seq_len)\n",
        "\n",
        "            # compute accuracy only at active labels\n",
        "            active_ner_accuracy = ner_labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "            ac_ner_labels = torch.masked_select(flattened_ner_targets, active_ner_accuracy)\n",
        "            ner_predictions = torch.masked_select(flattened_ner_predictions, active_ner_accuracy)\n",
        "\n",
        "            eval_ner_labels.extend(ac_ner_labels)\n",
        "            eval_ner_preds.extend(ner_predictions)\n",
        "\n",
        "            # compute accuracy for intent_model\n",
        "            # I CAN MAKE THE CALCULATION MUCH EASIER\n",
        "            # FIGURE IT OUT\n",
        "            flattened_intent_targets = intent_labels.view(-1)\n",
        "            active_intent_logits = intent_logits.logits.view(-1, intent_model.num_labels)\n",
        "            flattened_intent_predictions = torch.argmax(active_intent_logits, axis=1)\n",
        "\n",
        "            sample_intent_accuracy = intent_labels.view(-1)\n",
        "            active_intent_accuracy = torch.ones_like(sample_intent_accuracy, dtype=torch.bool)\n",
        "\n",
        "            ac_intent_labels = torch.masked_select(flattened_intent_targets, active_intent_accuracy)\n",
        "            intent_predictions = torch.masked_select(flattened_intent_predictions, active_intent_accuracy)\n",
        "\n",
        "            eval_intent_labels.extend(ac_intent_labels)\n",
        "            eval_intent_preds.extend(intent_predictions)\n",
        "\n",
        "            tmp_eval_ner_accuracy = accuracy_score(ac_ner_labels.cpu().numpy(), ner_predictions.cpu().numpy())\n",
        "            tmp_eval_intent_accuracy = accuracy_score(ac_intent_labels.cpu().numpy(), intent_predictions.cpu().numpy())\n",
        "\n",
        "            eval_ner_accuracy += tmp_eval_ner_accuracy\n",
        "            eval_intent_accuracy += tmp_eval_intent_accuracy\n",
        "\n",
        "    v_ner_labels = [ids_to_labels_ner[id.item()] for id in eval_ner_labels]\n",
        "    v_ner_predictions = [ids_to_labels_ner[id.item()] for id in eval_ner_preds]\n",
        "\n",
        "    v_intent_labels = [ids_to_labels_intent[id.item()] for id in eval_intent_labels]\n",
        "    v_intent_predictions = [ids_to_labels_intent[id.item()] for id in eval_intent_preds]\n",
        "\n",
        "    v_ner_loss = eval_ner_loss / nb_eval_steps\n",
        "    v_intent_loss = eval_intent_loss / nb_eval_steps\n",
        "    eval_ner_accuracy = eval_ner_accuracy / nb_eval_steps\n",
        "    eval_intent_accuracy = eval_intent_accuracy / nb_eval_steps\n",
        "    print(f\"Validation NER loss: {v_ner_loss}\")\n",
        "    print(f\"Validation INTENT loss: {v_intent_loss}\")\n",
        "    print(f\"Validation NER accuracy: {eval_ner_accuracy}\")\n",
        "    print(f\"Validation INTENT accuracy: {eval_intent_accuracy}\")\n",
        "\n",
        "    return v_ner_labels, v_ner_predictions, v_intent_labels, v_intent_predictions"
      ],
      "metadata": {
        "id": "sSumzyRcf4bc"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v_ner_labels, v_ner_predictions, v_intent_labels, v_intent_predictions = eval(ner_model, intent_model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD4nCQVzpPep",
        "outputId": "12aacb7f-9b52-4604-ad58-0114fcdb591b"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation NER loss per 0 training steps: 0.5678038597106934\n",
            "Validation INTENT loss per 0 training steps: 0.8422508239746094\n",
            "Validation NER loss per 5 training steps: 0.7802771329879761\n",
            "Validation INTENT loss per 5 training steps: 1.184253215789795\n",
            "Validation NER loss per 10 training steps: 0.7801852226257324\n",
            "Validation INTENT loss per 10 training steps: 1.1666653156280518\n",
            "Validation NER loss per 15 training steps: 0.7583001852035522\n",
            "Validation INTENT loss per 15 training steps: 1.074487566947937\n",
            "Validation NER loss per 20 training steps: 0.7475306391716003\n",
            "Validation INTENT loss per 20 training steps: 1.0628502368927002\n",
            "Validation NER loss: 0.7646963000297546\n",
            "Validation INTENT loss: 1.066023349761963\n",
            "Validation NER accuracy: 0.7997146372146374\n",
            "Validation INTENT accuracy: 0.7083333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Mr_kyDspvN0"
      },
      "execution_count": 134,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}