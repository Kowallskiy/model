{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BG1_zZBjNuM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2dbd00b-c206-4783-c227-1382e30f0216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.5.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece] python-docx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast, BertForTokenClassification, BertForSequenceClassification, BertModel\n",
        "from transformers import BertConfig\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import TextDataset"
      ],
      "metadata": {
        "id": "uKo0d9ltN6aM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# Read the JSON file\n",
        "path = \"/content/drive/MyDrive/model/B_data.json\"\n",
        "with open(path, 'r') as f:\n",
        "    dataset = json.load(f)"
      ],
      "metadata": {
        "id": "NmR5din-N8Zq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/model/training_set.json\"\n",
        "with open(path, 'r') as f:\n",
        "    test_dataset = json.load(f)"
      ],
      "metadata": {
        "id": "ChyWzbcAOCAC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text, intent, ner = [], [], []\n",
        "for i in dataset:\n",
        "    text.append(i['text'])\n",
        "    intent.append(i['intent'])\n",
        "    ner.append(i['entities'].split())"
      ],
      "metadata": {
        "id": "nRzWOIZ7OCpa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text, test_intent, test_ner = [], [], []\n",
        "for i in test_dataset:\n",
        "    test_text.append(i['text'])\n",
        "    test_intent.append(i['intent'])\n",
        "    test_ner.append(i['entities'].split())"
      ],
      "metadata": {
        "id": "b-NXvXb9OELC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_intents = set(intent)\n",
        "num_intent_labels = len(unique_intents)\n",
        "\n",
        "unique_intents, num_intent_labels"
      ],
      "metadata": {
        "id": "TSqnFLXSOFpy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa0ec1f2-4ed2-4ed2-dbab-b11bd3eb79a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({\"'Schedule Appointment'\",\n",
              "  \"'Schedule Meeting'\",\n",
              "  \"'Set Alarm'\",\n",
              "  \"'Set Reminder'\",\n",
              "  \"'Set Timer'\"},\n",
              " 5)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_dimensional_ner = [tag for subset in ner for tag in subset ]\n",
        "unique_ner = set(one_dimensional_ner)\n",
        "num_ner_labels = len(unique_ner)\n",
        "unique_ner, num_ner_labels"
      ],
      "metadata": {
        "id": "Yq7QUTyAOHQC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ec39294-2d6a-4bd5-df0c-530d854c1883"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'B-DATE',\n",
              "  'B-DUR',\n",
              "  'B-TASK',\n",
              "  'B-TIME',\n",
              "  'I-DATE',\n",
              "  'I-DUR',\n",
              "  'I-TASK',\n",
              "  'I-TIME',\n",
              "  'O'},\n",
              " 9)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "ner_model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_ner_labels)\n",
        "intent_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_intent_labels)\n",
        "bert_backbone = BertModel.from_pretrained('bert-base-uncased')\n",
        "config = BertConfig.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "XawZZRogOI_y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7194a2c-0f67-405f-f837-e5257ee3d54a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_to_ids_ner = {\n",
        "    'O': 0,\n",
        "    'B-DATE': 1,\n",
        "    'I-DATE': 2,\n",
        "    'B-TIME': 3,\n",
        "    'I-TIME': 4,\n",
        "    'B-TASK': 5,\n",
        "    'I-TASK': 6,\n",
        "    'B-DUR': 7,\n",
        "    'I-DUR': 8\n",
        "    }\n",
        "\n",
        "ids_to_labels_ner = {v: k for k, v in labels_to_ids_ner.items()}\n",
        "ids_to_labels_ner"
      ],
      "metadata": {
        "id": "oIOr8LgdOMFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb1655c5-d842-442d-d72b-96915e25917c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'O',\n",
              " 1: 'B-DATE',\n",
              " 2: 'I-DATE',\n",
              " 3: 'B-TIME',\n",
              " 4: 'I-TIME',\n",
              " 5: 'B-TASK',\n",
              " 6: 'I-TASK',\n",
              " 7: 'B-DUR',\n",
              " 8: 'I-DUR'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_to_ids_intent = {\n",
        "    \"'Schedule Appointment'\": 0,\n",
        "    \"'Schedule Meeting'\": 1,\n",
        "    \"'Set Alarm'\": 2,\n",
        "    \"'Set Reminder'\": 3,\n",
        "    \"'Set Timer'\": 4\n",
        "}\n",
        "\n",
        "ids_to_labels_intent = {v: k for k, v in labels_to_ids_intent.items()}\n",
        "ids_to_labels_intent"
      ],
      "metadata": {
        "id": "WP5okVE_OOwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35c0fef8-abe0-4de9-e0e7-614b2a21998d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: \"'Schedule Appointment'\",\n",
              " 1: \"'Schedule Meeting'\",\n",
              " 2: \"'Set Alarm'\",\n",
              " 3: \"'Set Reminder'\",\n",
              " 4: \"'Set Timer'\"}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class dataset(Dataset):\n",
        "    def __init__(self, text, intent, ner, tokenizer, max_len=128):\n",
        "        self.len = len(text)\n",
        "        self.text = text\n",
        "        self.intent = intent\n",
        "        self.ner = ner\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # step 1: get the sentence, ner label, and intent_label\n",
        "        sentence = self.text[index].strip()\n",
        "        intent_label = self.intent[index].strip()\n",
        "        ner_labels = self.ner[index]\n",
        "\n",
        "        # step 2: use tokenizer to encode a sentence (includes padding/truncation up to max length)\n",
        "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" which highlights where each token starts and ends\n",
        "        encoding = self.tokenizer(\n",
        "            sentence,\n",
        "            return_offsets_mapping=True,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_len\n",
        "        )\n",
        "\n",
        "        # step 3: create ner token labels only for first word pieces of each tokenized word\n",
        "        tokenized_ner_labels = [labels_to_ids_ner[label] for label in ner_labels]\n",
        "        # create an empty array of -100 of length max_length\n",
        "        encoded_ner_labels = np.ones(len(encoding['offset_mapping']), dtype=int) * -100\n",
        "\n",
        "        # set only labels whose first offset position is 0 and the second is not 0\n",
        "        i = 0\n",
        "        prev = -1\n",
        "        for idx, mapping in enumerate(encoding['offset_mapping']):\n",
        "            if mapping[0] == mapping[1] == 0:\n",
        "                continue\n",
        "            if mapping[0] != prev:\n",
        "                # overwrite label\n",
        "                encoded_ner_labels[idx] = tokenized_ner_labels[i]\n",
        "                prev = mapping[1]\n",
        "                i += 1\n",
        "            else:\n",
        "                prev = mapping[1]\n",
        "\n",
        "        # create intent token labels\n",
        "        tokenized_intent_label = labels_to_ids_intent[intent_label]\n",
        "\n",
        "        # step 4: turn everything into Pytorch tensors\n",
        "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
        "        item['ner_labels'] = torch.as_tensor(encoded_ner_labels)\n",
        "        item['intent_labels'] = torch.as_tensor(tokenized_intent_label)\n",
        "\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "GD2hhkYWOQiy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = dataset(text, intent, ner, tokenizer)\n",
        "test_set = dataset(test_text, test_intent, test_ner, tokenizer)"
      ],
      "metadata": {
        "id": "QzZaSdx0OT38"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[20]['input_ids']), training_set[20]['ner_labels']):\n",
        "    print(f\"{token} -- {label}\")"
      ],
      "metadata": {
        "id": "qlHH71-3OWd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cc596f6-7a93-4d5c-c418-8d76deab54fa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] -- -100\n",
            "schedule -- 0\n",
            "a -- 0\n",
            "dentist -- 5\n",
            "appointment -- 6\n",
            "for -- 0\n",
            "april -- 1\n",
            "5th -- 2\n",
            "at -- 0\n",
            "11 -- 3\n",
            ": -- -100\n",
            "00 -- -100\n",
            "in -- 4\n",
            "the -- 4\n",
            "morning -- 4\n",
            ". -- -100\n",
            "[SEP] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset is small, batch_size of 1 would not impact the training time significantly\n",
        "training_loader = DataLoader(training_set, batch_size=1)\n",
        "test_loader = DataLoader(test_set, batch_size=1)"
      ],
      "metadata": {
        "id": "O7CtBSAROZla"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "UhinzuBnOcKr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "qRtTRygxOdpd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "065eca75-8981-4c16-d149-0c88288bcba8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ner_model.to(device)\n",
        "intent_model.to(device)"
      ],
      "metadata": {
        "id": "D80nUJfwOesz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b8e74fa-8028-4cd3-f888-bcd6c86f3258"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intent_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxg924LJTy5h",
        "outputId": "5e468d12-2e49-4998-80bd-5fd38c665cf9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ner_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsZV41f7Si_o",
        "outputId": "63436dcc-c0d7-4f2f-df80-c94b9f11888c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_backbone"
      ],
      "metadata": {
        "id": "h6DomtlwIssm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ebe1abb-f4aa-4297-84c2-ef16a726df68"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTq1Iwe_iIE8",
        "outputId": "8627997a-0172-4a4f-f9b9-b199ade426ed"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"architectures\": [\n",
              "    \"BertForMaskedLM\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.35.2\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam([\n",
        "    {'params': ner_model.parameters()},\n",
        "    {'params': intent_model.parameters()}\n",
        "], lr=1e-5)"
      ],
      "metadata": {
        "id": "-kVD2zy8OoOD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi Task Learning Arcitecture"
      ],
      "metadata": {
        "id": "ESSOrSEdOr_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskBertModel(BertModel):\n",
        "\n",
        "    \"\"\"\n",
        "    Multi-task Bert model for Named Entity Recognition (NER) and Intent Classification\n",
        "\n",
        "    Args:\n",
        "        config (BertConfig): Bert model configuration.\n",
        "        num_ner_labels (int): The number of labels for NER task.\n",
        "        num_intent_labels (int): The number of labels for Intent Classification task.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, num_ner_labels, num_intent_labels):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.num_ner_labels = num_ner_labels\n",
        "        self.num_intent_labels = num_intent_labels\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        self.ner_classifier = torch.nn.Linear(config.hidden_size, self.num_ner_labels)\n",
        "        self.intent_classifier = torch.nn.Linear(config.hidden_size, self.num_intent_labels)\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None,\n",
        "                ner_labels=None, intent_labels=None):\n",
        "\n",
        "        \"\"\"\n",
        "        Perform a forward pass through Multi-task Bert model.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): Input token IDs.\n",
        "            attention_mask (torch.Tensor): Attention mask for input tokens.\n",
        "            ner_labels (torch.Tensor): Labels for NER task.\n",
        "            intent_labels (torch.Tensor): Labels for Intent Classification task.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]: NER logits or Intent logits.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If neither ner_labels nor intent_labels were provided.\n",
        "        \"\"\"\n",
        "\n",
        "        if ner_labels is not None:\n",
        "            outputs = super().forward(input_ids=input_ids,\n",
        "                                      attention_mask=attention_mask)\n",
        "            sequence_output = outputs[0]\n",
        "            sequence_output = self.dropout(sequence_output)\n",
        "            ner_logits = self.ner_classifier(sequence_output)\n",
        "\n",
        "            ner_loss_fct = torch.nn.CrossEntropyLoss()\n",
        "            ner_loss = ner_loss_fct(ner_logits.view(-1, self.num_ner_labels), ner_labels.view(-1))\n",
        "\n",
        "        if intent_labels is not None:\n",
        "            outputs = super().forward(input_ids=input_ids,\n",
        "                                      attention_mask=attention_mask)\n",
        "            pooled_output = outputs[1]\n",
        "            pooled_output = self.dropout(pooled_output)\n",
        "            intent_logits = self.intent_classifier(pooled_output)\n",
        "\n",
        "            intent_loss_fct = torch.nn.CrossEntropyLoss()\n",
        "            intent_loss = intent_loss_fct(intent_logits.view(-1, self.num_intent_labels), intent_labels.view(-1))\n",
        "\n",
        "        if ner_labels is None or intent_labels is None:\n",
        "            raise ValueError(\"Either ner_labels or intent_labels were not provided.\")\n",
        "\n",
        "        return ner_loss, ner_logits, intent_loss, intent_logits\n"
      ],
      "metadata": {
        "id": "LfEsaNqpOuwO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiTaskBertModel(config, num_ner_labels, num_intent_labels)"
      ],
      "metadata": {
        "id": "hbNCz7ROYds9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    tr_ner_loss, tr_ner_accuracy = 0, 0\n",
        "    tr_intent_loss, tr_intent_accuracy = 0, 0\n",
        "    nb_tr_steps = 0\n",
        "    tr_ner_preds, tr_ner_labels = [], []\n",
        "    tr_intent_labels, tr_intent_predictions = [], []\n",
        "    model.train()\n",
        "\n",
        "\n",
        "    for idx, batch in enumerate(training_loader):\n",
        "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "        ner_labels = batch['ner_labels'].to(device, dtype=torch.long)\n",
        "        intent_labels = batch['intent_labels'].to(device, dtype=torch.long)\n",
        "\n",
        "        ner_loss, ner_logits, intent_loss, intent_logits = model(ids, mask, ner_labels, intent_labels)\n",
        "\n",
        "        comb_loss = ner_loss + intent_loss\n",
        "\n",
        "        tr_ner_loss += ner_loss.item()\n",
        "        tr_intent_loss += intent_loss.item()\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "        if idx % 5 == 0:\n",
        "            loss_ner_step = tr_ner_loss / nb_tr_steps\n",
        "            loss_intent_step = tr_intent_loss / nb_tr_steps\n",
        "            print(f\"Training NER loss per {idx} training steps: {loss_ner_step}\")\n",
        "            print(f\"Training INTENT loss per {idx} training steps: {loss_intent_step}\")\n",
        "\n",
        "        # compute training accuracy (FOR NER)\n",
        "        flattened_ner_targets = ner_labels.view(-1) # shape (batch_size * seq_len)\n",
        "        active_ner_logits = ner_logits.view(-1, num_ner_labels) # shape (batch_size*seq_len, num_labels)\n",
        "        flattened_ner_predictions = torch.argmax(active_ner_logits, axis=1) # shape (batch_size * seq_len)\n",
        "\n",
        "        # compute accuracy only at active labels\n",
        "        active_ner_accuracy = ner_labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        ac_ner_labels = torch.masked_select(flattened_ner_targets, active_ner_accuracy)\n",
        "        ner_predictions = torch.masked_select(flattened_ner_predictions, active_ner_accuracy)\n",
        "\n",
        "        tr_ner_labels.extend(ac_ner_labels)\n",
        "        tr_ner_preds.extend(ner_predictions)\n",
        "\n",
        "        # compute accuracy for intent_model\n",
        "        # I CAN MAKE THE CALCULATION MUCH EASIER\n",
        "        # FIGURE IT OUT\n",
        "        flattened_intent_targets = intent_labels.view(-1)\n",
        "        active_intent_logits = intent_logits.view(-1, num_intent_labels)\n",
        "        flattened_intent_predictions = torch.argmax(active_intent_logits, axis=1)\n",
        "\n",
        "        sample_intent_accuracy = intent_labels.view(-1)\n",
        "        active_intent_accuracy = torch.ones_like(sample_intent_accuracy, dtype=torch.bool)\n",
        "\n",
        "        ac_intent_labels = torch.masked_select(flattened_intent_targets, active_intent_accuracy)\n",
        "        intent_predictions = torch.masked_select(flattened_intent_predictions, active_intent_accuracy)\n",
        "\n",
        "        tr_intent_labels.extend(ac_intent_labels)\n",
        "        tr_intent_predictions.extend(intent_predictions)\n",
        "\n",
        "        tmp_tr_ner_accuracy = accuracy_score(ac_ner_labels.cpu().numpy(), ner_predictions.cpu().numpy())\n",
        "        tmp_tr_intent_accuracy = accuracy_score(ac_intent_labels.cpu().numpy(), intent_predictions.cpu().numpy())\n",
        "\n",
        "\n",
        "        tr_ner_accuracy += tmp_tr_ner_accuracy\n",
        "        tr_intent_accuracy += tmp_tr_intent_accuracy\n",
        "\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters = model.parameters(), max_norm=10\n",
        "        )\n",
        "\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        comb_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = (tr_ner_loss + tr_intent_loss) / nb_tr_steps\n",
        "    tr_ner_accuracy = tr_ner_accuracy / nb_tr_steps\n",
        "    tr_intent_accuracy = tr_intent_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training NER accuracy epoch: {tr_ner_accuracy}\")\n",
        "    print(f\"Training INTENT accuracy epoch: {tr_intent_accuracy}\")"
      ],
      "metadata": {
        "id": "a_4NQi94bouA"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3):\n",
        "    print(f\"Training epoch: {epoch+1}\")\n",
        "    print(\"-----------------------------\")\n",
        "    train(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzpU5DIxcGp6",
        "outputId": "01bff7d4-6134-4a86-b92b-633ec83821d0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch: 1\n",
            "-----------------------------\n",
            "Training NER loss per 0 training steps: 2.1673552989959717\n",
            "Training INTENT loss per 0 training steps: 1.6900688409805298\n",
            "Training NER loss per 5 training steps: 2.1874396602312722\n",
            "Training INTENT loss per 5 training steps: 1.6025922497113545\n",
            "Training NER loss per 10 training steps: 2.258472616022283\n",
            "Training INTENT loss per 10 training steps: 1.559900630604137\n",
            "Training NER loss per 15 training steps: 2.317053332924843\n",
            "Training INTENT loss per 15 training steps: 1.6326571330428123\n",
            "Training NER loss per 20 training steps: 2.2916664509546187\n",
            "Training INTENT loss per 20 training steps: 1.6293905008406866\n",
            "Training NER loss per 25 training steps: 2.3174009048021755\n",
            "Training INTENT loss per 25 training steps: 1.6521743215047395\n",
            "Training NER loss per 30 training steps: 2.301511802980977\n",
            "Training INTENT loss per 30 training steps: 1.6308588866264588\n",
            "Training loss epoch: 3.9352242367608206\n",
            "Training NER accuracy epoch: 0.09088625660054232\n",
            "Training INTENT accuracy epoch: 0.11428571428571428\n",
            "Training epoch: 2\n",
            "-----------------------------\n",
            "Training NER loss per 0 training steps: 2.3399057388305664\n",
            "Training INTENT loss per 0 training steps: 1.8705906867980957\n",
            "Training NER loss per 5 training steps: 2.26436440149943\n",
            "Training INTENT loss per 5 training steps: 1.6807055274645488\n",
            "Training NER loss per 10 training steps: 2.274324883114208\n",
            "Training INTENT loss per 10 training steps: 1.654054663398049\n",
            "Training NER loss per 15 training steps: 2.298049159348011\n",
            "Training INTENT loss per 15 training steps: 1.7136531099677086\n",
            "Training NER loss per 20 training steps: 2.2830167214075723\n",
            "Training INTENT loss per 20 training steps: 1.6998527844746907\n",
            "Training NER loss per 25 training steps: 2.3003442883491516\n",
            "Training INTENT loss per 25 training steps: 1.748780525647677\n",
            "Training NER loss per 30 training steps: 2.2894355289397703\n",
            "Training INTENT loss per 30 training steps: 1.7330548128774088\n",
            "Training loss epoch: 4.018312306063516\n",
            "Training NER accuracy epoch: 0.11421657707371992\n",
            "Training INTENT accuracy epoch: 0.11428571428571428\n",
            "Training epoch: 3\n",
            "-----------------------------\n",
            "Training NER loss per 0 training steps: 2.632794141769409\n",
            "Training INTENT loss per 0 training steps: 1.8445121049880981\n",
            "Training NER loss per 5 training steps: 2.2329993645350137\n",
            "Training INTENT loss per 5 training steps: 1.6797868808110554\n",
            "Training NER loss per 10 training steps: 2.2450698722492564\n",
            "Training INTENT loss per 10 training steps: 1.5993954051624646\n",
            "Training NER loss per 15 training steps: 2.287654787302017\n",
            "Training INTENT loss per 15 training steps: 1.6478680670261383\n",
            "Training NER loss per 20 training steps: 2.2744868028731573\n",
            "Training INTENT loss per 20 training steps: 1.633866826693217\n",
            "Training NER loss per 25 training steps: 2.2934340146871715\n",
            "Training INTENT loss per 25 training steps: 1.6330842559154217\n",
            "Training NER loss per 30 training steps: 2.2858697791253366\n",
            "Training INTENT loss per 30 training steps: 1.6303193838365617\n",
            "Training loss epoch: 3.9233708449772426\n",
            "Training NER accuracy epoch: 0.10391013748156608\n",
            "Training INTENT accuracy epoch: 0.11428571428571428\n"
          ]
        }
      ]
    }
  ]
}